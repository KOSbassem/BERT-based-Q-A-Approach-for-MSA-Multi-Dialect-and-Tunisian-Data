{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOjjxaUgU-8z"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "file_path = '/content/tydiqa-goldp-dev-arabic.json'\n",
        "data = load_json(file_path)\n",
        "print(f\"Nombre total d'éléments : {len(data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# Utiliser le chemin complet du modèle téléchargé localement\n",
        "model_path = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# Charger le modèle et le tokenizer NER pour AraBERTv2\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# Pipeline NER pour extraire les entités nommées\n",
        "nlp_ner = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer)\n"
      ],
      "metadata": {
        "id": "7At2s1NXVUBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour extraire les entités nommées d'un texte\n",
        "def extract_ner_entities(text, nlp_ner):\n",
        "    ner_results = nlp_ner(text)\n",
        "    entities = [result['word'] for result in ner_results]\n",
        "    return entities"
      ],
      "metadata": {
        "id": "mJNEhkbH0JkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features_and_filter_with_ner(examples, tokenizer, max_length=512):\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for article in examples['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            context_entities = extract_ner_entities(context, nlp_ner)  # Appliquer NER sur le contexte\n",
        "\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                question_entities = extract_ner_entities(question, nlp_ner)  # Appliquer NER sur la question\n",
        "                answer = qa['answers'][0]\n",
        "                start_char = answer['answer_start']\n",
        "                end_char = start_char + len(answer['text'])\n",
        "\n",
        "                # Ajouter les entités extraites au contexte et à la question\n",
        "                enriched_context = context + \" \" + \" \".join(context_entities)\n",
        "                enriched_question = question + \" \" + \" \".join(question_entities)\n",
        "\n",
        "                # Encodage avec tokenizer\n",
        "                encoded = tokenizer(\n",
        "                    enriched_question,\n",
        "                    enriched_context,\n",
        "                    padding='max_length',\n",
        "                    max_length=max_length,\n",
        "                    truncation=\"only_second\",\n",
        "                    return_offsets_mapping=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                offsets = encoded['offset_mapping'][0]\n",
        "                token_start_index = 0\n",
        "                token_end_index = len(offsets) - 1\n",
        "\n",
        "                # Ajuster les index de début et fin pour la réponse\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][1] <= start_char:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                while token_end_index >= 0 and offsets[token_end_index][0] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Stocker les données uniquement si elles sont bien alignées\n",
        "                if token_start_index < len(offsets) and token_end_index >= 0:\n",
        "                    contexts.append(enriched_context)\n",
        "                    questions.append(enriched_question)\n",
        "                    start_positions.append(token_start_index)\n",
        "                    end_positions.append(token_end_index)\n",
        "\n",
        "    # Encodage final\n",
        "    encodings = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    encodings.update({\n",
        "        'start_positions': torch.tensor(start_positions, dtype=torch.long),\n",
        "        'end_positions': torch.tensor(end_positions, dtype=torch.long)\n",
        "    })\n",
        "\n",
        "    return encodings\n"
      ],
      "metadata": {
        "id": "1hX1gN-bVT-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Charger le modèle et le tokenizer AraBERTv2 pour la tâche de question-réponse (QA)\n",
        "arabertv2_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabertv2_model = AutoModelForQuestionAnswering.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# Charger les encodages avec NER et préparer le DataLoader\n",
        "train_encodings_with_ner = prepare_train_features_and_filter_with_ner(data, arabertv2_tokenizer)\n",
        "train_dataset_with_ner = QADataset(train_encodings_with_ner)\n",
        "train_dataloader = DataLoader(train_dataset_with_ner, batch_size=8, shuffle=True)\n",
        "\n",
        "# Configuration du modèle et de l'optimisation\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "arabertv2_model.to(device)\n",
        "optimizer = AdamW(arabertv2_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(3):\n",
        "    arabertv2_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "        outputs = arabertv2_model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Époque {epoch+1}/3 - Perte Moyenne: {total_loss / len(train_dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "9PIHxZat01R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(dataloader, model):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            start_preds = torch.argmax(outputs.start_logits, dim=-1)\n",
        "\n",
        "            all_labels.extend(start_positions.cpu().numpy())\n",
        "            all_preds.extend(start_preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "# Évaluer le modèle\n",
        "evaluate_model(train_dataloader, arabertv2_model)\n"
      ],
      "metadata": {
        "id": "y5Inkc2YVbGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "import torch\n",
        "\n",
        "# Fonction d'évaluation pour calculer accuracy, recall et F1-score après chaque époque\n",
        "def evaluate_model(dataloader, model):\n",
        "    model.eval()  # Mettre le modèle en mode évaluation\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():  # Ne pas calculer les gradients pendant l'évaluation\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "\n",
        "            # Obtenir les prédictions du modèle\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            start_preds = torch.argmax(outputs.start_logits, dim=-1)\n",
        "\n",
        "            # Ajouter les prédictions et les labels à la liste\n",
        "            all_labels.extend(start_positions.cpu().numpy())\n",
        "            all_preds.extend(start_preds.cpu().numpy())\n",
        "\n",
        "    # Calculer accuracy, recall et F1-score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return accuracy, recall, f1\n",
        "\n",
        "# Boucle d'entraînement avec évaluation à chaque époque\n",
        "for epoch in range(5):\n",
        "    arabertv2_model.train()  # Mettre le modèle en mode entraînement\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "        # Calculer les prédictions et la perte\n",
        "        outputs = arabertv2_model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculer les gradients et optimiser\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculer la perte moyenne à la fin de chaque époque\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Évaluer le modèle après chaque époque\n",
        "    accuracy, recall, f1 = evaluate_model(train_dataloader, arabertv2_model)\n",
        "\n",
        "    # Afficher les résultats pour chaque époque\n",
        "    print(f\"Époque {epoch+1}/3 - Perte Moyenne: {avg_loss:.4f} - Accuracy: {accuracy:.4f} - Recall: {recall:.4f} - F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "jpKrONwe3JeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
        "\n",
        "# Charger le modèle et le tokenizer XLM-Roberta pour la tâche de question-réponse (QA)\n",
        "xlm_roberta_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "xlm_roberta_model = AutoModelForQuestionAnswering.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# Charger les encodages avec NER et préparer le DataLoader\n",
        "train_encodings_with_ner = prepare_train_features_and_filter_with_ner(data, xlm_roberta_tokenizer)\n",
        "train_dataset_with_ner = QADataset(train_encodings_with_ner)\n",
        "train_dataloader = DataLoader(train_dataset_with_ner, batch_size=8, shuffle=True)\n",
        "\n",
        "# Configuration du modèle et de l'optimisation\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "xlm_roberta_model.to(device)\n",
        "optimizer = AdamW(xlm_roberta_model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "id": "HvwdYXPL66eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "import torch\n",
        "\n",
        "# Fonction d'évaluation pour calculer accuracy, recall et F1-score après chaque époque\n",
        "def evaluate_model(dataloader, model):\n",
        "    model.eval()  # Mettre le modèle en mode évaluation\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():  # Ne pas calculer les gradients pendant l'évaluation\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "\n",
        "            # Obtenir les prédictions du modèle\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            start_preds = torch.argmax(outputs.start_logits, dim=-1)\n",
        "\n",
        "            # Ajouter les prédictions et les labels à la liste\n",
        "            all_labels.extend(start_positions.cpu().numpy())\n",
        "            all_preds.extend(start_preds.cpu().numpy())\n",
        "\n",
        "    # Calculer accuracy, recall et F1-score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return accuracy, recall, f1\n",
        "\n",
        "# Boucle d'entraînement avec évaluation à chaque époque\n",
        "for epoch in range(10):\n",
        "    xlm_roberta_model.train()  # Mettre le modèle en mode entraînement\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "        # Calculer les prédictions et la perte\n",
        "        outputs = xlm_roberta_model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculer les gradients et optimiser\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculer la perte moyenne à la fin de chaque époque\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Évaluer le modèle après chaque époque\n",
        "    accuracy, recall, f1 = evaluate_model(train_dataloader, xlm_roberta_model)\n",
        "\n",
        "    # Afficher les résultats pour chaque époque\n",
        "    print(f\"Époque {epoch+1}/3 - Perte Moyenne: {avg_loss:.4f} - Accuracy: {accuracy:.4f} - Recall: {recall:.4f} - F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "2JkTFB5X-Amk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1EWrfXQ0BFK2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}