{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcXZ5g4DrK11",
        "outputId": "2ed280c3-def2-4a3e-87f6-bfc29212c14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Dossier contenant les fichiers CSV à traiter\n",
        "dossier_csv = '/content/drive/MyDrive/dataNett'\n",
        "\n",
        "# Initialiser une liste pour stocker tous les DataFrames corrigés\n",
        "dataframes_corriges = []\n",
        "\n",
        "# Fonction pour nettoyer les balises et espaces\n",
        "def nettoyer_texte(texte):\n",
        "    if isinstance(texte, str):\n",
        "        texte = re.sub(r'\\[DQ\\]', '', texte)\n",
        "        texte = re.sub(r'\\[FQ\\]', '', texte)\n",
        "        texte = re.sub(r'\\[DR\\]', '', texte)\n",
        "        texte = re.sub(r'\\[FR\\]', '', texte)\n",
        "        texte = texte.strip()\n",
        "    return texte\n",
        "\n",
        "# Itérer sur chaque fichier CSV dans le dossier\n",
        "for fichier in os.listdir(dossier_csv):\n",
        "    if fichier.endswith(\".csv\"):\n",
        "        # Lire le fichier CSV\n",
        "        chemin_csv = os.path.join(dossier_csv, fichier)\n",
        "        df = pd.read_csv(chemin_csv)\n",
        "\n",
        "        # Nettoyer les colonnes du DataFrame\n",
        "        df['Contexte'] = df['Contexte'].apply(nettoyer_texte)\n",
        "        df['Question'] = df['Question'].apply(nettoyer_texte)\n",
        "        df['Réponse'] = df['Réponse'].apply(nettoyer_texte)\n",
        "\n",
        "        # Supprimer les lignes où 'Contexte', 'Question', ou 'Réponse' est vide\n",
        "        df = df[df['Contexte'].str.strip() != \"\"]\n",
        "        df = df[df['Question'].str.strip() != \"\"]\n",
        "        df = df[df['Réponse'].str.strip() != \"\"]\n",
        "\n",
        "        # Ajouter le DataFrame corrigé à la liste\n",
        "        dataframes_corriges.append(df)\n",
        "\n",
        "# Combiner tous les DataFrames corrigés dans un DataFrame global\n",
        "df_final_global = pd.concat(dataframes_corriges, ignore_index=True)\n",
        "\n",
        "# Sauvegarder le DataFrame final nettoyé dans un fichier CSV\n",
        "df_final_global.to_csv('/content/drive/MyDrive/data_final_nettoye.csv', index=False)\n",
        "\n",
        "# Afficher quelques exemples pour vérifier le contenu\n",
        "print(df_final_global.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pSF5w_3gYNR",
        "outputId": "3893ca27-6b46-4084-87ea-151610065fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Contexte  \\\n",
            "0  لاء باش يحافظوا على ه العضو هذا ويحافظوا على ص...   \n",
            "1  ولوا اشربوا ياسر ماء وه عندهم انتفاخ وعندهم تر...   \n",
            "2  وا ياسر ماء ولكن حسب الوضعيه تعهم نقولوا لهم ا...   \n",
            "3  تغذيه الكلوه بالدم وهذا ينجم نجر عليه نوع من ا...   \n",
            "4  الدم والا ادويه اع مضاده للالتهابات هذوما ثروا...   \n",
            "\n",
            "                                            Question  \\\n",
            "0  كيفاش هالارتفاع المفرد في درجات الحراره وخاصه ...   \n",
            "1  الفئات الناس اللي عندها الحساء يلزم ترد بالها ...   \n",
            "2  العلامات دكتور والاعراض ماما قاعدين نشوف العلا...   \n",
            "3  الفئات الاكثر عرضه دكتور باي الجفاف هذا كيفاش ...   \n",
            "4  بالنسبه ل عندهم زرع دكتور اللي سناو في زرع ما ...   \n",
            "\n",
            "                                             Réponse  \n",
            "0  الكلاء ماما قلت انت هو عضو اساسي في البدن في ج...  \n",
            "1  ينجم يكون لانه هو الماء يق كونسيون تعزين بع ال...  \n",
            "2  ارتفاع في درج الحراره بصفه عامه بنقص كميه البو...  \n",
            "3  المضاعفات هي ما قلتلك هي في مرحله متقدمه معنات...  \n",
            "4  بالنسبه اللي زارعين الكلا اللي زارعين الكلاء د...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chemin vers le fichier CSV individuel\n",
        "fichier_csv = '/content/drive/MyDrive/data_final_nettoye.csv'  # Remplacez par le chemin correct\n",
        "\n",
        "# Charger les données du fichier CSV\n",
        "df_final_global = pd.read_csv(fichier_csv)\n",
        "\n",
        "# Afficher les premières lignes pour vérifier le contenu\n",
        "print(\"Aperçu des données :\")\n",
        "df_final_global"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "XkehBDng5Aza",
        "outputId": "6e3eea47-776f-464d-c813-2a3a7eaff9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aperçu des données :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Contexte  \\\n",
              "0    لاء باش يحافظوا على ه العضو هذا ويحافظوا على ص...   \n",
              "1    ولوا اشربوا ياسر ماء وه عندهم انتفاخ وعندهم تر...   \n",
              "2    وا ياسر ماء ولكن حسب الوضعيه تعهم نقولوا لهم ا...   \n",
              "3    تغذيه الكلوه بالدم وهذا ينجم نجر عليه نوع من ا...   \n",
              "4    الدم والا ادويه اع مضاده للالتهابات هذوما ثروا...   \n",
              "..                                                 ...   \n",
              "105  واء المناسب للحاله بتاعه هو من نو خروش على ذكر...   \n",
              "106  حظوظين دونك حنا نحاول نشخص المرض  بطريقه بيزت ...   \n",
              "107  يني باش يطرق لهم المؤتمر المتوسطي للطب الباطني...   \n",
              "108  يع امراض برشا امراض صحيح دكتوره جهاد زده من بي...   \n",
              "109  ا موجودين معانا و مع الوبه توينس اللي بكونوا ا...   \n",
              "\n",
              "                                              Question  \\\n",
              "0    كيفاش هالارتفاع المفرد في درجات الحراره وخاصه ...   \n",
              "1    الفئات الناس اللي عندها الحساء يلزم ترد بالها ...   \n",
              "2    العلامات دكتور والاعراض ماما قاعدين نشوف العلا...   \n",
              "3    الفئات الاكثر عرضه دكتور باي الجفاف هذا كيفاش ...   \n",
              "4    بالنسبه ل عندهم زرع دكتور اللي سناو في زرع ما ...   \n",
              "..                                                 ...   \n",
              "105                           شنو هو العلاج اع مرض بهج   \n",
              "106  نرجع للنصائح الى جانب التشخيص الدقيق والحين وا...   \n",
              "107  علاش اخترتوا ه النظام الغذائي هذايا وشنو منفعت...   \n",
              "108  علاش حتى نحكي على مؤتمر متوسطي للطب الباطني وب...   \n",
              "109    حوصله مرض بهجه شنو تنجم تزيد تحكينا على النصائح   \n",
              "\n",
              "                                               Réponse  \n",
              "0    الكلاء ماما قلت انت هو عضو اساسي في البدن في ج...  \n",
              "1    ينجم يكون لانه هو الماء يق كونسيون تعزين بع ال...  \n",
              "2    ارتفاع في درج الحراره بصفه عامه بنقص كميه البو...  \n",
              "3    المضاعفات هي ما قلتلك هي في مرحله متقدمه معنات...  \n",
              "4    بالنسبه اللي زارعين الكلا اللي زارعين الكلاء د...  \n",
              "..                                                 ...  \n",
              "105  علاج في مرض بهجت فيه زوج دو فولي فما لو تريتم ...  \n",
              "106  ننصح المريض انه اول حاجه يواظب على دواه ما يقص...  \n",
              "107  الريجيم هذايا هو حاجه ورثناها من عند جدوده موج...  \n",
              "108  السؤال يطرح نفسه تبقى امراض السكر وغض الدم وام...  \n",
              "109  مرض بهجه هو موجود في تونس نخم فيه ك تبدا فم ل ...  \n",
              "\n",
              "[110 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7de0812-01ec-497f-9790-8396a86ec38c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Contexte</th>\n",
              "      <th>Question</th>\n",
              "      <th>Réponse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>لاء باش يحافظوا على ه العضو هذا ويحافظوا على ص...</td>\n",
              "      <td>كيفاش هالارتفاع المفرد في درجات الحراره وخاصه ...</td>\n",
              "      <td>الكلاء ماما قلت انت هو عضو اساسي في البدن في ج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ولوا اشربوا ياسر ماء وه عندهم انتفاخ وعندهم تر...</td>\n",
              "      <td>الفئات الناس اللي عندها الحساء يلزم ترد بالها ...</td>\n",
              "      <td>ينجم يكون لانه هو الماء يق كونسيون تعزين بع ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>وا ياسر ماء ولكن حسب الوضعيه تعهم نقولوا لهم ا...</td>\n",
              "      <td>العلامات دكتور والاعراض ماما قاعدين نشوف العلا...</td>\n",
              "      <td>ارتفاع في درج الحراره بصفه عامه بنقص كميه البو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>تغذيه الكلوه بالدم وهذا ينجم نجر عليه نوع من ا...</td>\n",
              "      <td>الفئات الاكثر عرضه دكتور باي الجفاف هذا كيفاش ...</td>\n",
              "      <td>المضاعفات هي ما قلتلك هي في مرحله متقدمه معنات...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الدم والا ادويه اع مضاده للالتهابات هذوما ثروا...</td>\n",
              "      <td>بالنسبه ل عندهم زرع دكتور اللي سناو في زرع ما ...</td>\n",
              "      <td>بالنسبه اللي زارعين الكلا اللي زارعين الكلاء د...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>واء المناسب للحاله بتاعه هو من نو خروش على ذكر...</td>\n",
              "      <td>شنو هو العلاج اع مرض بهج</td>\n",
              "      <td>علاج في مرض بهجت فيه زوج دو فولي فما لو تريتم ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>حظوظين دونك حنا نحاول نشخص المرض  بطريقه بيزت ...</td>\n",
              "      <td>نرجع للنصائح الى جانب التشخيص الدقيق والحين وا...</td>\n",
              "      <td>ننصح المريض انه اول حاجه يواظب على دواه ما يقص...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>يني باش يطرق لهم المؤتمر المتوسطي للطب الباطني...</td>\n",
              "      <td>علاش اخترتوا ه النظام الغذائي هذايا وشنو منفعت...</td>\n",
              "      <td>الريجيم هذايا هو حاجه ورثناها من عند جدوده موج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>يع امراض برشا امراض صحيح دكتوره جهاد زده من بي...</td>\n",
              "      <td>علاش حتى نحكي على مؤتمر متوسطي للطب الباطني وب...</td>\n",
              "      <td>السؤال يطرح نفسه تبقى امراض السكر وغض الدم وام...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>ا موجودين معانا و مع الوبه توينس اللي بكونوا ا...</td>\n",
              "      <td>حوصله مرض بهجه شنو تنجم تزيد تحكينا على النصائح</td>\n",
              "      <td>مرض بهجه هو موجود في تونس نخم فيه ك تبدا فم ل ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7de0812-01ec-497f-9790-8396a86ec38c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7de0812-01ec-497f-9790-8396a86ec38c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7de0812-01ec-497f-9790-8396a86ec38c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ee7b12a0-3471-4be4-953c-d047ea7657ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee7b12a0-3471-4be4-953c-d047ea7657ae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ee7b12a0-3471-4be4-953c-d047ea7657ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_155c6cc3-7c76-4bc4-8872-039255db89f4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_final_global')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_155c6cc3-7c76-4bc4-8872-039255db89f4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_final_global');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final_global",
              "summary": "{\n  \"name\": \"df_final_global\",\n  \"rows\": 110,\n  \"fields\": [\n    {\n      \"column\": \"Contexte\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 108,\n        \"samples\": [\n          \"\\u0646 \\u0627\\u0644\\u0648\\u0627\\u0644\\u0647 \\u062a\\u0639\\u062f\\u0627\\u0648 \\u0627\\u0644\\u0645\\u0641\\u0631\\u0648\\u0636 \\u0646\\u062c\\u0645 \\u0646\\u0643\\u0648\\u0646\\u0648\\u0627  \\u0635\\u0639\\u0627\\u0628 \\u0634\\u0648\\u064a \\u0639\\u0644\\u0649 \\u0645\\u0631\\u0636\\u0649 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u062c\\u0645\\u0639\\u062a\\u064a\\u0646 \\u062a\\u0639\\u062f\\u0627\\u0648 \\u0627\\u062d\\u0646\\u0627 \\u0641\\u064a \\u0627\\u0644\\u062c\\u0645\\u0639\\u0647 \\u0627\\u0644\\u062b\\u0627\\u0644\\u062b\\u0647 \\n\\u0634\\u0646 \\u0647\\u064a \\u0627\\u062d\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0639\\u064a\\u0627\\u062f\\u0627\\u062a \\u0648\\u0634\\u0646 \\u0647\\u064a \\u0627\\u062d\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0641\\u064a \\u0627\\u0644\\u0641\\u062a\\u0631\\u0647 \\u0647\\u0630\\u0647 \\u064a\\u0639\\u0637\\u064a\\u0643 \\u0627\\u0644\\u0635\\u062d\\u0647 \\u0627\\u0646 \\u0634\\u0627\\u0621 \\u0627\\u0644\\u0644\\u0647 \\u0631\\u0645\\u0636\\u0627\\u0646 \\u0645\\u0628\\u0631\\u0648\\u0643 \\u0648\\u0642\\u0631\\u064a\\u0628 \\u0634\\u0627\\u0631\\u0641 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0646\\u0647\\u0627\\u064a\\u0647 \\u0627\\u0644\\u0645\\u0633\\u062a\\u0645\\u0639\\u064a\\u0646 \\u0627\\u0644\\u0645\\u0634\\u0627\\u0647\\u062f\\u064a\\u0646 \\u0627\\u0644\\u0643\\u0644 \\u062a\\u0639 \\u0627\\u0644\\u0642\\u0646\\u0627\\u0647 \\u0627\\u0644\\u0648\\u0637\\u0646\\u064a\\u0647 \\u0631\\u0645\\u0636\\u0627\\u0646 \\u0628\\u062e\\u064a\\u0631 \\u0645\\u0646 \\u0627\\u0644\\u0639\\u0648\\u0627\\u0645 \\u0627\\u0644\\u0627\\u062e\\u0631\\u0649 \\u064a\\u0638\\u0647\\u0631\\u0644\\u064a \\u0639\\u0644\\u0649 \\u062e\\u0627\\u0637\\u0631 \\u0627\\u0644\\u0637\\u0642\\u0633 \\u0645\\u0646 \\u0627\\u0644\\u0627\\u0648\\u0644 \\u062c\\u0627\\u064a \\u0628\\u0627\\u0631\\u062f \\u0628\\u0627\\u0647\\u064a \\u0627\\u0644\\u0639\\u0628\\u0627\\u062f \\u062f\\u062e\\u0644\\u062a \\u0641\\u064a\\u0633\",\n          \"\\u0642\\u0627\\u0647 \\u0647\\u0645 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0627\\u0644\\u062f\\u0646 \\u062a\\u0639\\u0647\\u0645 \\u0645\\u0627\\u0634\\u064a \\u0634\\u0648\\u064a\\u0647 \\u0644\\u0644\\u062c\\u0641\\u0627\\u0641 \\u0646\\u0642\\u0635\\u0648\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0627\\u062f\\u0648\\u064a\\u0647 \\u0627\\u0644\\u0645\\u062f\\u0631\\u0647 \\u0644\\u0644\\u0628\\u0648\\u0644 \\u0641\\u064a \\u0641\\u062a\\u0631\\u0647 \\u0627\\u0644\\u0635\\u064a\\u0641 \\u0648\\u0628\\u0646\\u0633\\u0628\\u0647  \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0644 \\u0627\\u0644\\u0646\\u0638\\u0627\\u0645 \\u0627\\u0644\\u063a\\u0630\\u0627\\u0626\\u064a \\u0627\\u0644\\u0646\\u0638\\u0627\\u0645 \\u0627\\u0644\\u063a\\u0630\\u0627\\u0626\\u064a \\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647  \\u0644\\u0648\\u0636\\u0639\\u064a\\u0647 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0628\\u062e\\u0644\\u0627\\u0641 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0647\\u0648 \\u0627\\u0643\\u062b\\u0631 \\u062d\\u0627\\u062c\\u0647 \\u0646\\u0628\\u0647\\u0648\\u0627 \\u0645\\u0646\\u0647\\u0627 \\u0648 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u062a\\u0628\\u0639\\u062f \\u0639\\u0644\\u064a\\u0647\\u0627 \\u0647\\u064a \\u0627\\u0644\\u0628\\u0631\\u0648\\u062a\\u064a\\u0646\\u0627\\u062a \\u0627\\u0644\\u062d\\u064a\\u0648\\u0627\\u0646\\u064a\\u0647 \\u0648\\u062e\\u0627\\u0635\\u0647 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0627\\u0644\\u062d\\u0645\\u0631\\u0627\\u0621 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0645\\u0639\\u0646\\u0627\\u062a\\u0647\\u0627 \\u0627\\u0643\\u062b\\u0631 \\u062d\\u0627\\u062c\\u0627\\u062a \\u0627\\u0644\\u0644\\u064a \\u062a\\u0646\\u062c\\u0645 \\u062a\\u0627\\u062b\\u0631 \\u0633\\u0644\\u0628\\u064a\\u0627 \\u0639\\u0644\\u0649 \\u0648\\u0638\\u0627\\u0626\\u0641 \\u0627\\u0644\\u0643\\u0644\\u0627\",\n          \"\\u0627\\u0644\\u062f\\u0645 \\u0648\\u0627\\u0644\\u0627 \\u0627\\u062f\\u0648\\u064a\\u0647 \\u0627\\u0639 \\u0645\\u0636\\u0627\\u062f\\u0647 \\u0644\\u0644\\u0627\\u0644\\u062a\\u0647\\u0627\\u0628\\u0627\\u062a \\u0647\\u0630\\u0648\\u0645\\u0627 \\u062b\\u0631\\u0648\\u0627 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u062a\\u063a\\u0630\\u064a\\u0647 \\u0628\\u0639 \\u0627\\u0644\\u0634\\u0631\\u0627\\u064a\\u064a\\u0646 \\u0628\\u0639\\u0639 \\u0627\\u0644\\u0643\\u0644\\u0627\\u0628 \\u062f\\u0643\\u062a\\u0648\\u0631    \\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0644 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0632\\u0631\\u0639 \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0627\\u0644\\u0644\\u064a \\u0633\\u0646\\u0627\\u0648 \\u0641\\u064a \\u0632\\u0631\\u0639 \\u0645\\u0627 \\u0643\\u064a\\u0641\\u0627\\u0634  \\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0627\\u0644\\u0644\\u064a  \\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0627\\u0644\\u0644\\u064a \\u0632\\u0627\\u0631\\u0639\\u064a\\u0646 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0627\\u0644\\u0644\\u064a \\u0632\\u0627\\u0631\\u0639\\u064a\\u0646 \\u0627\\u0644\\u0643\\u0644\\u0627\\u0621 \\u062f\\u0648\\u0643\\u0645 \\u0647\\u064a \\u0639\\u0628\\u0627\\u0631\\u0647 \\u0643\\u0644\\u0648\\u0647 \\u0648\\u0627\\u062d\\u062f\\u0647 \\u062a\\u0634\\u062a\\u063a\\u0644 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0646\\u0627\\u0633 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u0642\\u0648\\u0644 \\u0644\\u0647\\u0645 \\u064a\\u0644\\u0632\\u0645\\u0643\\u0645 \\u062a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0645\\u0627 \\u0643\\u0645\\u064a\\u0647 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u0645\\u0646\\u0647\\u0627\\u0631 \\u0646\\u0632\\u0631\\u0639 \\u0648\\u0627\\u062d\\u0646\\u0627 \\u0646\\u0628\\u062f\\u0627 \\u0646\\u0648\\u0635 \\u064a\\u0648\\u0647\\u0645 \\u0639\\u0627\\u062f\\u0647 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0628\\u064a\\u0646 \\u0644\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 108,\n        \"samples\": [\n          \"\\u0643\\u064a\\u0641\\u0627\\u0634 \\u062f\\u0643\\u062a\\u0648\\u0631\\u0632 \\u0648 \\u0627\\u0644\\u0627\\u0644\\u0627\\u0645 \\u0643\\u064a\\u0641\\u0627\\u0634 \\u0646\\u062a\\u0639\\u0627\\u0645\\u0644 \\u0645\\u0639\\u0647\\u0627 \\u0641\\u064a \\u0634\\u0647\\u0631 \\u0631\\u0645\\u0636\\u0627\\u0646 \\u0627\\u0648 \\u0643\\u064a\\u0641\\u0627\\u0634 \\u062a\\u0639\\u0627\\u0645\\u0644 \\u0645\\u0639\\u0647\\u0627 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0648\\u0643\\u064a\\u0641\\u0627\\u0634 \\u064a\\u062a\\u0639\\u0627\\u0645\\u0644 \\u0645\\u0639\\u0647\\u0627     [RD]\\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u0627\\u0644\\u0643\\u0648\\u064a \\u0646\\u0639\\u0631\\u0641\\u0648\\u0647\\u0627 \\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u062c\\u0631\\u0647 \\u0627\\u0644\\u062d\\u062c\\u0631 \\u0635\\u0641\\u0647 \\u0639\\u0627\\u0645\\u0647 \\u062a\\u062c \\u062a\\u0643\\u0648\\u0646 \\u0627\\u0633\\u0628\\u0627\\u0628 \\u0627\\u062e\\u0631\\u0649 90 \\u0641\\u064a \\u064a\\u0642\\u0648\\u0644 \\u0627\\u062d\\u0646\\u0627 \\u0627\\u0644\\u0645\\u0633\\u0628\\u0627\\u062a \\u0627\\u0644\\u062d\\u062c\\u0631 \\u062d\\u0635 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u0645\\u0639\\u0631\\u0648\\u0641\\u0647 \\u0645\\u0646 \\u0627\\u0642\\u0648\\u0649 \\u0627\\u0644\\u0648\\u062c\\u0627\\u0639 \\u062a\\u062c\\u064a \\u0639\\u0646\\u062f \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0627\\u0644\\u0631\\u064a \\u0627\\u0644\\u0627\\u0644\\u0645\\u0639 \\u0627\\u0644\\u0643\\u0644\\u0648 \\u064a\\u062c\\u064a \\u0639\\u0634 \\u064a\\u0642\\u0648\\u0644\\u0643 \\u0645\\u0646 \\u0627\\u0642\\u0648\\u0649 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u064a\\u0627\\u0633\\u0631 \\u064a\\u062d\\u0633\\u0647\\u0627 \\u0627\\u0644\\u0645\\u0633\\u0627\\u0646 \\u0639\\u0644\\u0649 \\u062e\\u0627\\u0637\\u0631 \\u062f\\u064a\\u0645\\u0627 \\u064a\\u0642\\u0639\\u062f \\u0639\\u0646\\u062f\\u0647 \\u0627\\u0644\\u0647\\u0627\\u062c\\u0633 \\u0627\\u0646\\u064a \\u062a\\u0631\\u062c\\u0639\\u0644\\u0647 \\u0646\\u0647\\u0627\\u0631 \\u0627\\u062e\\u0631 \\u0644\\u0645\\u0648\\u0646 \\u0644\\u062c\\u0639\\u0647 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u062a\\u062c\\u064a \\u0641\\u064a \\u0627\\u0633\\u0641\\u0644 \\u0641\\u064a \\u0627\\u0644\\u0638\\u0647\\u0631 \\u0641\\u064a \\u0627\\u0633\\u0647\\u0644 \\u0627\\u0644\\u0638\\u0647\\u0631 \\u0639\\u0644\\u0649 \\u062c\\u0646\\u0628 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0628\\u0627\\u0644\\u0636\\u0628\\u0637 \\u0646\\u0648\\u0631\\u064a \\u0641\\u064a \\u0645\\u0643\\u0627\\u0646 \\u0647\\u0630\\u0627\\u064a\\u0627 \\u0648\\u0627\\u0644\\u0644\\u064a \\u062a\\u062a\\u0631\\u0633\\u0628 \\u0644\\u062c\\u0639\\u0647 \\u0648\\u062a\\u0646\\u062c\\u0645 \\u062a\\u0647\\u0628\\u0637 \\u062d\\u062a\\u0649 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0633\\u0627\\u0642 \\u0634\\u0648\\u064a\\u0647 \\u0627\\u0644\\u0648\\u062c\\u064a\\u0639\\u0647 \\u062a\\u062c\\u064a \\u0643\\u0627\\u0646\\u064a \\u0636\\u0631\\u0628\\u062a \\u062e\\u0646\\u062c\\u0631 \\u0645\\u0639 \\u0648\\u062c\\u064a\\u0639\\u0647 \\u0642\\u0648\\u064a\\u0647 \\u064a\\u0627\\u0633\\u0631 \\u0645\\u0646 \\u063a\\u064a\\u0631 \\u0645\\u0627 \\u062a\\u0639\\u0637\\u064a\\u0643 \\u0627\\u0639\\u0631\\u0627\\u0621 \\u0645\\u0646 \\u063a\\u064a\\u0631 \\u0645\\u0627 \\u062a\\u0642\\u0648\\u0644\\u0644\\u0643 \\u0631\\u0627 \\u0641 \\u0648\\u062c\\u064a\\u0639\\u0647 \\u062a\\u062c\\u064a\\u0643 \\u062a\\u062c\\u064a \\u0643\\u0631\\u064a\\u0632 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u0630\\u0627\\u0643 \\u0639\\u0634 \\u0627\\u0644\\u0639\\u0628\\u0627\\u062f \\u0628\\u0635\\u0641\\u0647 \\u0639\\u0627\\u0645\\u0647 \\u064a\\u0645\\u0634\\u064a \\u0644\\u0648\\u0646\\u0633 \\u0645\\u0641\\u062c \\u0645\\u0627\\u064a\\u0639\\u0631\\u0641\\u0648 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0648\\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u0647\\u0630 \\u062a\\u0645\\u0634\\u064a \\u062a\\u0645\\u0634\\u064a \\u062a\\u0639\\u0637\\u064a \\u0636\\u0631\\u0628\\u0647 \\u0642\\u0648\\u064a\\u0647 \\u062a\\u0647\\u0641 \\u0634\\u0648\\u064a\\u0647 \\u0648\\u064a\\u0642\\u0639\\u062f \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0645\\u0633\\u0643\\u064a\\u0646 \\u064a\\u0628\\u062f\\u0627 \\u064a\\u0633\\u062a\\u0646\\u0649 \\u0648\\u0642\\u0634 \\u062a\\u062c\\u064a \\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u0627\\u0644\\u0644\\u064a \\u0628\\u0639\\u062f\\u0647\\u0627 \\u062a\\u0648\\u062c\\u0639 \\u0628\\u0631\\u0634\\u0627 \\u0628\\u0631\\u0634\\u0627\\u0643 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0646\\u0646\\u0635\\u062d \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u0646\\u0648\\u0639 \\u0627\\u0644\\u0627\\u0645 \\u0647\\u0630\\u0627 \\u0645\\u0634 \\u062f\\u064a\\u0645\\u0627 \\u0647\\u064a \\u062a\\u0635\\u064a\\u0631 \\u0647\\u0630 \\u063a\\u0644\\u0628\\u064a\\u0647 \\u064a\\u0643\\u064a\\u0643 \\u0647\\u064a \\u062a\\u0635\\u064a\\u0631 \\u062a\\u0642\\u0648\\u0644\\u0646\\u0627 \\u062d\\u0635\\u0648\\u0647 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0648 \\u062b\\u0645 \\u0639\\u0628\\u0627\\u062f \\u064a\\u062a\\u0634\\u0643 \\u0628\\u0634\\u0648\\u064a\\u0647 \\u0627\\u0648\\u062c\\u0627\\u0639 \\u0648\\u0644\\u0627 \\u0634\\u0648\\u064a\\u0647 \\u0642\\u0644\\u0642 \\u0627\\u0644\\u0644\\u064a \\u0646\\u0646\\u0635\\u062d\\u0647\\u0645 \\u064a\\u0645\\u064a \\u0641\\u064a \\u0639\\u0642\\u0644\\u0647\\u0645 \\u064a\\u0639\\u062f\\u064a\\u0648 \\u064a\\u0634\\u0648\\u0641\\u0648\\u0627 \\u0627\\u0645\\u0627 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u0643\\u0631\\u064a\\u0632 \\u062f \\u0643\\u0648\\u0644\\u064a\\u0643 \\u0646\\u062a\\u064a\\u0643 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u0627\\u0644\\u0643\\u0631\\u064a\\u0632 \\u062a\\u0627\\u0639 \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0647\\u0630 \\u0631\\u0627 \\u064a\\u0644\\u0632\\u0645 \\u064a\\u0645\\u0634\\u064a \\u0627\\u0644\\u0627\\u0633\\u062a\\u0639\\u062c\\u0627\\u0644\\u064a \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0645\\u0627\\u0647\\u0648\\u0634 \\u0645\\u0634\\u0643\\u0644\\u0647 \\u0645\\u0634 \\u0646\\u0639\\u0627\\u0644\\u062c \\u0641\\u064a \\u0648\\u0642\\u062a\\u0647\\u0627 \\u0627\\u0644\\u062d\\u0635\\u0648\\u0647 \\u0645\\u0634 \\u0646\\u062d\\u064a \\u0627\\u0644\\u0633\\u0628\\u0628 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0627\\u0642\\u0644\\u0645 \\u0647\\u0630\\u0627\\u0643 \\u0627\\u0644\\u0627\\u0644\\u0645 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u064a\\u0645\\u0634\\u064a \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u062c\\u0627\\u062a\\u0648 \\u0643\\u0631\\u064a\\u0632 \\u0646\\u0635\\u062d\\u0648\\u0647 \\u064a\\u0645\\u0634\\u064a \\u0627\\u064a \\u0627\\u0633\\u062a\\u0639\\u062c\\u0627\\u0644 \\u0633\\u0648\\u0627 \\u062e\\u0627\\u0635 \\u0645\\u0633\\u062a\\u0634\\u0641\\u064a\\u0627\\u062a \\u0627\\u0644\\u0639\\u0645\\u0648\\u0645\\u064a\\u0647 \\u0645\\u0648\\u062c\\u0648\\u062f\\u064a\\u0646 \\u0646\\u0639\\u0631\\u0641\\u0648\\u0627 \\u0646\\u062a\\u0639\\u0627\\u0645\\u0644 \\u0645\\u0639\\u0647\\u0645 \\u0646\\u0627\\u062e\\u0630 \\u0627\\u0644\\u062f\\u0648\\u0627\\u0644 \\u0627\\u0644\\u0644\\u0627\\u0632\\u0645 \\u0648\\u0645\\u0646 \\u0628\\u0639\\u062f \\u0646\\u0627\\u062e\\u0630 \\u0631\\u064a\\u0642\\u0647 \\u0645\\u0646 \\u0639\\u0646\\u062f \\u0627\\u0644\\u0627\\u0633\\u062a\\u0639\\u062c\\u0627\\u0644\\u064a \\u0628\\u0627\\u0634 \\u0646\\u0645\\u0634\\u064a \\u0646\\u0634\\u0648\\u0641\\u0648\\u0627 \\u0637\\u0628\\u064a\\u0628 \\u0627\\u0644\\u0627\\u062e\\u062a\\u0635\\u0627\\u0635 \\u0646\\u062b\\u0628\\u062a \\u0634\\u0646\\u0648 \\u0627\\u0644\\u0633\\u0628\\u0628 \\u0627\\u0639\\u0647\\u0627 \\u0627\\u0644\\u062d\\u0635\\u0648\\u0647 \\u0648\\u064a\\u0646 \\u0648\\u0645 \\u0628\\u0639\\u062f \\u0628\\u0627\\u0644\\u0627\\u062e\\u062a\\u0635\\u0627\\u0635 \\u0645\\u0627 \\u0627\\u062d\\u0646\\u0627 \\u062c\\u0631\\u0627\\u062d\\u0647 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0644\\u0644\\u0647 \\u0646\\u062c\\u0645\\u0648\\u0627 \\u0646\\u0635\\u0628\\u0631\\u0648\\u0627 \\u0639\\u0644\\u064a\\u0647\\u0627 \\u0646\\u062c\\u0645\\u0648\\u0627 \\u0646\\u0635\\u0628\\u0631 \\u0634\\u0648\\u064a\\u0647 \\u0645\\u0627\\u0632\\u0645 \\u062a\\u0645\\u0634\\u064a \\u0639\\u0644\\u0649 \\u0631\\u0648\\u062d\\u0647\\u0627 \\u0648\\u0644\\u0627 \\u064a\\u0644\\u0632\\u0645\\u0647\\u0627 \\u0639\\u0644\\u0627\\u062c \\u062a\\u062f\\u062e\\u0644  \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0627\\u0634 \\u0628\\u062f\\u0646\\u0627 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0642\\u0644\\u062a \\u064a\\u062c\\u064a \\u0628\\u0634 \\u0642 \\u0627\\u0644\\u0641\\u062a\\u0631 \\u0648\\u0634\\u0631\\u0628\\u0647\\u0645 \\u0627\\u0644\\u0643\\u0644 \\u0628\\u0639\\u0636\\u0647\\u0645 \\u0642\\u062f\\u0627\\u0634 \\u064a\\u0644\\u0632\\u0645\\u0648\\u0627 \\u0645\\u0631\\u064a\\u0636 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0645\\u0646 \\u0634\\u0631\\u0628 \\u0645\\u0646 \\u0646\\u064a\\u062a\\u0631\\u0627 \\u0645\\u0646 \\u0645\\u0627\\u0621 \\u0648\\u0642\\u062f\\u0627\\u0634 \\u064a\\u0642\\u0633\\u0645\\u0647\\u0645 \\u0639\\u0644\\u0649 \\u0641\\u062a\\u0631\\u0627\\u062a \\u0648\\u0644\\u0627 \\u0647\\u0648 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636    \\u0646\\u062d\\u0643\\u064a \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0628\\u0627\\u0644\\u0643\\u0644\\u0627 \\u0648 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0627\\u0644\\u0639\\u0627\\u062f\\u064a \\u0645\\u0631\\u064a\\u0636 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0645\\u0631\\u064a\\u0636 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0634\\u0646\\u0648 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u062d\\u0633\\u0648\\u0647 \\u062d\\u0635\\u0648\\u0647 \\u062d \\u0628\\u0635\\u0641\\u0647 \\u0639\\u0627\\u0645\\u0647 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0646\\u0639\\u0631\\u0641\\u0648\\u0647 \\u0642\\u0628\\u0644 \\u0631\\u0645\\u0636 \\u0648\\u064a\\u0639\\u0631\\u0641 \\u0631\\u0648\\u062d\\u0647 \\u0639\\u0646\\u062f\\u0647 \\u062d\\u0635\\u0648\\u0647 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0645\\u0627\\u0630\\u0627 \\u0628\\u0647 \\u064a\\u0645\\u0634\\u064a \\u0644\\u0644\\u0637\\u0628\\u064a\\u0628 \\u0627\\u0639\\u0647 \\u064a\\u0642\\u0648\\u0644\\u0647 \\u0646\\u0635\\u0648\\u0645 \\u0648\\u0644\\u0627 \\u0645\\u0627 \\u0646\\u0635\\u0648\\u0645 \\u0631\\u0627 \\u0645\\u0634 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u0627\\u0644\\u0643\\u0644 \\u0643\\u064a\\u0641 \\u0643\\u064a\\u0641 \\u0645\\u0634 \\u0627\\u0644\\u062d\\u0635\\u0648\\u0627\\u062a \\u0627\\u0644\\u0643\\u0644 \\u0643\\u064a\\u0641 \\u0643\\u064a\\u0641 \\u062b\\u0645 \\u0627\\u0644\\u062d\\u0635\\u0648\\u0627\\u062a \\u0627\\u0644\\u0644\\u064a \\u0641\\u064a\\u0647 \\u0645\\u062e\\u062a\\u0631 \\u062d\\u0635\\u0648\\u0627\\u062a \\u062e\\u0637\\u0631 \\u0641\\u064a \\u0631\\u0645\\u0636\\u0627\\u0646 \\u0627\\u0630\\u0627 \\u0643\\u0627\\u0646 \\u0646\\u0635\\u0648\\u0645 \\u0646\\u0647\\u0627\\u0631 \\u0643\\u0627\\u0645\\u0644 \\u0648\\u0645\\u0627 \\u0646\\u0634\\u0631\\u0628\\u0648 \\u0628\\u0631\\u0634\\u0627 \\u0645\\u0627 \\u0627\\u0644\\u062d\\u062c\\u0631\\u0647 \\u062a\\u0646\\u062c\\u0645 \\u062a\\u0643\\u0628\\u0631 \\u0631\\u0627\\u0632 \\u0646\\u0639\\u0631\\u0641 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0628\\u0635\\u0641\\u0647 \\u0639\\u0627\\u0645\\u0647 \\u0639\\u0646\\u062f \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0645\\u0631\\u064a\\u0636 \\u0628\\u0627\\u0644\\u0643\\u0644\\u0627 \\u0648\\u0644\\u0627 \\u0639\\u0646\\u062f\\u0647 \\u062d\\u0633\\u0648\\u0647 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0632\\u0645 \\u064a\\u062a\\u0634\\u0631\\u0628 \\u0639\\u0644\\u0649 \\u0637\\u0648\\u0644 \\u0627\\u0644\\u0646\\u0647\\u0627\\u0631 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u0646\\u062a\\u0638\\u0645\\u0647 \\u0645\\u0646 \\u0627\\u0644\\u0635\\u0628\\u0627\\u062d \\u0644\\u0644\\u064a\\u0644 \\u0631\\u0627 \\u0627\\u0630\\u0627 \\u0643\\u0627\\u0646 \\u0646\\u0642\\u0635\\u0648\\u0627 \\u0646\\u0634\\u0631\\u0628\\u0648 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0641\\u062a\\u0631\\u0647 \\u0627\\u0644\\u062d\\u062c\\u0631\\u0647 \\u0647\\u064a \\u0628\\u064a\\u062f\\u0647\\u0627 \\u062a\\u0646\\u062c\\u0645 \\u062a\\u064a\\u0631 \\u062a\\u064a\\u0631 \\u0641\\u0648\\u0642\\u0647\\u0627 \\u0628\\u0643\\u062a\\u064a\\u0631\\u064a\\u0627 \\u062a\\u0646\\u062c\\u0645 \\u062a\\u064a\\u0631 \\u062c\\u062b\\u0645 \\u0641\\u0648\\u0642\\u0647\\u0627 \\u0627\\u062d\\u0646\\u0627 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u062e\\u0637\\u0631 \\u0646\\u0639\\u0645\\u0647 \\u0631\\u0627 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0646\\u0634\\u0631\\u0628\\u0647 \\u064a\\u063a\\u0633\\u0644 \\u064a\\u063a\\u0633\\u0644 \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0648\\u064a\\u063a\\u0633\\u0644 \\u0627\\u0644\\u062d\\u0627\\u0644\\u0628 \\u0648\\u064a\\u063a\\u0633\\u0644 \\u0634\\u0643\\u0627\\u0644\\u0647 \\u0627\\u0644\\u0628\\u0648\\u0644\\u0647 \\u062d\\u062a\\u0649 \\u0643\\u0627\\u0646 \\u062b\\u0645 \\u062d\\u0627\\u062c\\u0627\\u062a \\u0646\\u062c\\u0645 \\u062a\\u0645\\u0634\\u064a \\u0627\\u0645\\u0627 \\u0627\\u0630\\u0627 \\u0643\\u0627\\u0646 \\u0646\\u0642\\u0639\\u062f \\u0627\\u0646\\u0627 \\u0643\\u0648\\u064a\\u0647 \\u0634\\u064a\\u062d\\u064a\\u0646 \\u0648\\u0641\\u064a \\u0648\\u0633\\u0637\\u0647\\u0645 \\u062d\\u062c\\u0631\\u0647 \\u0647\\u064a \\u0641\\u064a\\u0647\\u0627 \\u0628\\u0643\\u062a\\u064a\\u0631\\u064a\\u0627 \\u062a\\u0639\\u0645\\u0644 \\u0641\\u064a\\u0644\\u0645 \\u0646\\u062e\\u0644\\u064a \\u0641\\u0631\\u0635\\u0647 \\u0645\\u0627\\u0634\\u0631\\u0628 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u062a\\u0643\\u062b\\u0631 \\u0627\\u0643\\u062b\\u0631 \\u0648\\u062a\\u0648\\u0644\\u064a \\u062a\\u0636\\u0631 \\u0627\\u0643\\u062b\\u0631 \\u062a\\u0646\\u062c\\u0645 \\u0641\\u064a\\u0647\\u0627 \\u0636\\u0631\\u0631 \\u0627\\u0644\\u062d\\u062c\\u0631\\u0647 \\u062a\\u0643\\u0628\\u0631 \\u0627\\u0644\\u062d\\u062c\\u0631\\u0647 \\u062a\\u0646\\u062c\\u0645 \\u062a\\u062a\\u062d\\u0631\\u0643 \\u062f\\u0648\\u0646\\u0643 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u062d\\u0635 \\u0641\\u064a \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0641\\u0627\\u0635\\u0648\\u0646 \\u062c\\u064a\\u0646\\u0627\\u0644 \\u064a\\u0644\\u0632\\u0645 \\u064a\\u0645\\u0634\\u064a \\u0644\\u0634\\u0627\\u0648\\u0631 \\u0627\\u0644\\u0637\\u0628\\u064a\\u0628\\u0639\\u064a \\u064a\\u0642\\u0644\\u0647 \\u0646\\u0635\\u0648\\u0645 \\u0648\\u0644\\u0627 \\u0645\\u0627 \\u0646\\u0635\\u0648\\u0645 \\u062b \\u0628\\u0639\\u0636 \\u0627\\u0644\\u062d\\u0627\\u0644\\u0627\\u062a \\u0627\\u0644\\u0644\\u064a \\u062d\\u062c\\u0631\\u0647 \\u0635\\u063a\\u064a\\u0631\\u0647 \\u062c\\u0627\\u064a \\u0641\\u064a \\u0627\\u062e\\u0631 \\u0627\\u0644\\u0643\\u0644\\u0648\\u0647 \\u0646\\u0639\\u0631\\u0641\\u0648\\u0647\\u0627 \\u0644\\u0627\\u0628\\u0627\\u0633 \\u0646\\u062c\\u0645 \\u0646\\u0642\\u0648\\u0644 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0645\\u0627\\u0633\\u0627\\u0644\\u0634 \\u062a\\u0646\\u062c\\u0645 \\u062a\\u0635\\u0648\\u0645 \\u0648\\u062a\\u0631\\u062f \\u0628\\u0627\\u0644\\u0643 \\u0639\\u0644\\u0649 \\u0631\\u0648\\u062d\\u0643 \\u0646\\u0631\\u062c\\u0639\\u0644\\u0643 \\u0644\\u0633\\u0624\\u0627\\u0644\\u0643 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0643\\u0645\\u064a\\u0647 \\u0627\\u0639 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0631\\u0627 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u062d\\u0643\\u064a \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0643\\u0645\\u064a\\u0647 \\u0627\\u0644\\u0644\\u064a \\u062a\\u0634\\u0631\\u0628 \\u0648\\u0637\\u0628\\u064a\\u0627 \\u062f\\u064a\\u0645\\u0627 \\u0627\\u0644\\u0643\\u0645\\u064a\\u0647 \\u0627\\u0644\\u0644\\u064a \\u062a\\u0628\\u0648\\u0644\\u0647\\u0627 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0645\\u0627\\u0630\\u0627 \\u0628\\u0647 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u064a\\u062a\\u0628\\u0648\\u0644 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0627\\u0642\\u0644 \\u0627\\u062a\\u062a\\u064a\\u0646 \\u0645\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0646\\u0647\\u0627\\u0631 \\u062f\\u0648\\u0646\\u0643 \\u064a\\u0639\\u0648\\u0636\\u0647\\u0645 \\u062f\\u0648\\u0646\\u0643 \\u0644\\u0627\\u0632\\u0645 \\u064a\\u0639\\u0648\\u0636\\u0647\\u0645 \\u0627\\u0642\\u0644 \\u0634\\u064a \\u064a\\u0639\\u0648\\u0636 \\u0647\\u0648\\u0644 \\u0648\\u0644\\u0643\\u0646 \\u0639\\u0644\\u0649 \\u0641\\u062a\\u0631\\u0627\\u062a   \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0639\\u0644\\u0649 \\u0641\\u062a\\u0631\\u0627\\u062a \\u0645\\u0627 \\u0628\\u064a\\u0646 \\u0627\\u0644\\u0627\\u0641\\u0637\\u0627\\u0631 \\u0648\\u0627\\u0644\\u0633\\u062d\\u0648\\u0631 \\u0627\\u0644\\u0627\\u0641\\u0637\\u0627\\u0631 \\u0648\\u0627\\u0644\\u0633\\u062d\\u0648\\u0631\",\n          \"\\u062f\\u0643\\u062a\\u0648\\u0631 \\u0644 \\u0627\\u0644\\u0646\\u0638\\u0627\\u0645 \\u0627\\u0644\\u063a\\u0630\\u0627\\u0626\\u064a \\u0627\\u0644\\u0646\\u0638\\u0627\\u0645 \\u0627\\u0644\\u063a\\u0630\\u0627\\u0626\\u064a\",\n          \"\\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0644 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0632\\u0631\\u0639 \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0627\\u0644\\u0644\\u064a \\u0633\\u0646\\u0627\\u0648 \\u0641\\u064a \\u0632\\u0631\\u0639 \\u0645\\u0627 \\u0643\\u064a\\u0641\\u0627\\u0634\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R\\u00e9ponse\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 108,\n        \"samples\": [\n          \"\\u0637\\u0628\\u064a\\u0628 \\u0646\\u062d\\u0643\\u064a\\u0648 \\u0639\\u0644\\u0649 \\u0627\\u0645\\u0631\\u0627\\u0636 \\u0627\\u0644\\u0643\\u0644\\u0627\\u0621 \\u0627\\u0644\\u0644\\u064a \\u0645\\u0631\\u062e\\u0635 \\u0644\\u0647\\u0645 \\u0627\\u0646\\u0647\\u0645 \\u0645\\u0631\\u062e\\u0635 \\u0644\\u0647\\u0645 \\u0648\\u0635\\u062d\\u064a\\u062a \\u062f\\u064a\\u0645\\u0627 \\u0631\\u0627\\u0647 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0645\\u0627\\u0630\\u0627 \\u0628\\u0646\\u0627 \\u0646\\u0634\\u0631\\u0628\\u0647 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u0646\\u062a\\u0638\\u0645\\u0647 \\u0646\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0645\\u0646 \\u0627\\u0644\\u0627\\u0648\\u0644 \\u0646\\u0634\\u0631\\u0628 \\u0645\\u0646 \\u0627\\u0644\\u0627\\u0648\\u0644 \\u0628\\u0637\\u0628\\u064a\\u0639\\u0647 \\u0646\\u0628\\u0644 \\u0631\\u064a\\u0642\\u0646\\u0627 \\u062f\\u064a\\u0645\\u0627 \\u0645\\u0627 \\u0646\\u0641\\u0633\\u0648 \\u0627\\u0644\\u0645\\u0627\\u0643\\u0644\\u0647 \\u0627\\u0639\\u0646\\u0627 \\u0645\\u0627 \\u0646\\u0639\\u0628\\u064a \\u0643\\u0631\\u0634\\u0646\\u0627 \\u0628\\u0627\\u0644\\u0645\\u0627\\u0621 \\u0648 \\u0645\\u064a\\u0645 \\u0637\\u0648\\u0646 \\u0645\\u0627 \\u0646\\u0634\\u0631\\u0628\\u0648 \\u0632\\u064a\\u0627\\u062f\\u0647 \\u0646\\u0631\\u062c\\u0639 \\u062f\\u064a\\u0645\\u0627 \\u0627\\u0644\\u062d\\u0644\\u0648 \\u0627\\u0644\\u0644\\u064a \\u0642\\u0644\\u0646\\u0627 \\u0627\\u062d\\u0646\\u0627 \\u064a\\u0634\\u0631\\u0628 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0631\\u0627 \\u0645\\u0627\\u0647\\u0648\\u0634 \\u0639\\u0627\\u062f\\u0647 \\u0633\\u064a\\u0626\\u0647 \\u0645\\u0627\\u0643\\u0644\\u062a\\u0646\\u0627 \\u0645\\u0627\\u0644\\u062d\\u0647 \\u0633\\u064a\\u0626\\u0647 \\u0646\\u0634\\u0631\\u0628 \\u0627\\u0644\\u0645\\u0627\\u0644\\u062d \\u0648\\u0646\\u0634\\u0631\\u0628 \\u0627\\u0644\\u062d\\u0644\\u0648 \\u0631\\u0627\\u0646 \\u064a\\u0644\\u0632\\u0645\\u0646\\u064a \\u0646\\u0632\\u064a\\u062f \\u0646\\u0634\\u0631\\u0628 \\u0627\\u0643\\u062b\\u0631 \\u0645\\u0627\\u0621 \\u0628\\u0627 \\u0646\\u0631\\u0648\\u064a \\u0628\\u062f\\u0646\\u064a \\u062e \\u0633\\u064a\\u0646\\u0648 \\u0647\\u0630\\u0643 \\u0627\\u0644\\u0643\\u0644 \\u0628\\u0627\\u0634 \\u064a\\u062e\\u0631\\u062c \\u0645\\u0639 \\u0627\\u0644\\u0628\\u0648\\u0644\\u0647 \\u0648\\u064a\\u0642\\u0644\\u0642 \\u0642\\u0644\\u0642\\u0646\\u0627 \\u062f\\u0648\\u0646\\u0643 \\u0631\\u062f\\u0648\\u0627 \\u0628\\u0627\\u0644\\u0646\\u0627 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647 \\u064a\\u0639\\u0631\\u0641 \\u0631\\u0648\\u062d\\u0647 \\u0645\\u0631\\u0636 \\u0645\\u062e\\u0648\\u0644 \\u0644\\u064a\\u0647 \\u0631\\u0627 \\u0646\\u0628\\u0639\\u062f \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0645\\u0644\\u062d \\u0646\\u0628\\u0639\\u062f \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0633\\u0643\\u0631 \\u0648\\u0646\\u0634\\u0631\\u0628 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0628\\u0627\\u0646\\u062a\\u0638\\u0627\\u0645 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0633\\u0647\\u0631\\u064a\\u0647 \\u0646\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0641\\u0637\\u0648\\u0631 \\u0634\\u0648\\u064a\\u0647 \\u0646\\u062d\\u0637\\u0648\\u0627 \\u0627\\u0644\\u0628\\u0648\\u0632\\u0647 \\u0628\\u062d\\u0630\\u0627\\u0646\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0633\\u062d\\u0648\\u0631 \\u0646\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0646\\u0627\\u062e\\u0630 \\u0633\\u0648\\u0627\\u0626\\u0644 \\u0627\\u062e\\u0631\\u0649 \\u0645\\u0627\\u0644\\u0634 \\u0633\\u0648\\u0627\\u0626\\u0644 \\u062d\\u0644\\u064a\\u0628 \\u062f\\u064a \\u0646\\u0634\\u0631\\u0628 \\u0644\\u0627\\u0632\\u0645 \\u0646\\u0634\\u0631\\u0628 \\u0627\\u0644\\u0645\\u0627\\u0621FR]f] \\u062f\\u0643\\u062a\\u0648\\u0631 \\u062d\\u0627\\u0641\\u0638 \\u0634\\u0643\\u0648\\u0646 \\u0627\\u0644\\u0644\\u064a \\u064a\\u062a\\u0639\\u0628 \\u0627\\u0643\\u062b\\u0631 \\u0628\\u0639\\u062f \\u0627\\u0644\\u062c\\u0645\\u0639\\u062a\\u064a\\u0646 \\u064a\\u0639\\u0646\\u064a \\u062a\\u062e\\u0646\\u0627 \\u0641\\u064a \\u0627\\u0644\\u062c\\u0645\\u0639\\u0647 \\u0627\\u0644\\u062b\\u0627\\u0644\\u062b\\u0647 \\u0634\\u0643\\u0648\\u0646 \\u0627\\u0644\\u0644\\u064a \\u064a\\u062a\\u0639\\u0628 \\u0627\\u0643\\u062b\\u0631 \\u0645\\u0631\\u0636\\u0649 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0648\\u0627\\u0644\\u0639\\u064a\\u0627\\u062f\\u0627\\u062a \\u0641\\u064a \\u0634\\u0631\\u0646\\u064a\\u0643 \\u062e\\u0644\\u064a\\u0646\\u0627 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0645\\u0631\\u062c\\u0644\\u064a\\u0646 \\u0645\\u0627\\u0645\\u0627 \\u0642\\u0627\\u0644 \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0633\\u0627\\u0645\\u064a \\u0628\\u0631\\u062d\\u0648\\u0645\\u0647 \\u064a\\u0639\\u0646\\u064a \\u0627\\u062e\\u0641 \\u0627\\u0644\\u0627\\u0636\\u0631\\u0627\\u0631 \\u0645\\u0646 \\u0639\\u0645\\u0646\\u0627 \\u0648\\u0627\\u0644\\u0633\\u0646\\u0627 \\u0648\\u0644\\u0627 \\u0641\\u0645 \\u0641\\u0631\\u0642   \\u0647\\u0648 \\u0646\\u062d\\u0643\\u064a \\u0639\\u0644\\u0649 \\u0627\\u0644\\u062c\\u0627\\u0646\\u0628 \\u0627\\u0644\\u0627\\u062e\\u0631 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0627\\u0645\\u0631\\u0627\\u0636 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u062e\\u0627\\u0635\\u0647 \\u0642\\u0635\\u0648\\u0631 \\u0641\\u064a \\u0648\\u0638\\u0627\\u0626\\u0641 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0639\\u0627\\u062f\\u0647 \\u0627\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0639\\u062f\\u064a\\u062f \\u0627\\u0644\\u0627\\u0645\\u0631\\u0627\\u0636 \\u0627\\u0644\\u0627\\u062e\\u0631\\u0649 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0627\\u0631\\u062a\\u0641\\u0627\\u0639 \\u0636\\u063a\\u0637 \\u0627\\u0644\\u062f\\u0645 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0633\\u0643\\u0631\\u064a \\u0645\\u0639\\u0647\\u0645 \\u0633\\u0643\\u0631\\u064a \\u0641\\u064a \\u0627\\u0644\\u0639\\u0627\\u062f\\u0647 \\u0627\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0627\\u0644\\u0645\\u0646\\u0638\\u0645\\u064a\\u0646 \\u0648\\u0627\\u0644\\u0644\\u064a \\u064a\\u0642\\u0648\\u0645\\u0648\\u0627 \\u0628\\u0627\\u0644\\u0639\\u062f\\u0627\\u062f \\u0628\\u062a\\u0627\\u0639\\u0647\\u0645 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u0646\\u062a\\u0638\\u0645\\u0647 \\u0646\\u0634\\u0648\\u0641 \\u0627\\u0644\\u062d\\u0642\\u064a \\u0627\\u063a\\u0644\\u0628\\u0647\\u0645 \\u064a\\u0633\\u0644\\u0648\\u0627 \\u0642\\u0628\\u0644 \\u0631\\u0645\\u0636\\u0627\\u0646 \\u064a\\u062c\\u064a \\u064a\\u0639\\u0645\\u0644\\u0648\\u0627 \\u0639 \\u064a\\u062d\\u0636\\u0631\\u0643 \\u0627\\u0644\\u0639\\u064a\\u0627\\u062f\\u0627\\u062a \\u0627\\u0644\\u0644\\u064a \\u0645\\u0627 \\u0642\\u0628\\u0644 \\u0631\\u0645\\u0636\\u0627\\u0646 \\u0648\\u0627\\u0636\\u062d\\u0647 \\u0642\\u0644\\u0647\\u0645 \\u0645\\u0634 \\u0645\\u0645\\u0643\\u0646 \\u0628\\u0627\\u0634 \\u062a\\u0635\\u0648\\u0645 \\u0645\\u0634 \\u0645\\u0645\\u0643\\u0646 \\u0628\\u0627\\u0634 \\u062a\\u0635\\u0648\\u0645 \\u0643\\u0645\\u0627 \\u0645\\u0631\\u0636 \\u0627\\u0644\\u0633\\u0643\\u0631\\u064a \\u064a\\u0627\\u062e\\u0630\\u0648\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0627\\u0646\\u0633\\u0648\\u0644\\u064a\\u0646 \\u0648\\u0639\\u0646\\u062f\\u0647\\u0645 \\u0642\\u0635\\u0648\\u0631 \\u0643\\u0630\\u0627 \\u0643\\u0645\\u0627 \\u0645\\u0631\\u0636 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0642\\u0635\\u0648\\u0631 \\u0643\\u0644\\u0627 \\u0641\\u064a \\u0627\\u0644\\u0645\\u0631\\u062d\\u0644\\u0647 \\u0627\\u0644\\u0631\\u0627\\u0628\\u0639\\u0647 \\u0648\\u0627\\u0644\\u062e\\u0627\\u0645\\u0633\\u0647 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0646\\u0627\\u0633 \\u0645\\u0627 \\u0645\\u0634 \\u0645\\u0633\\u0645\\u0648\\u062d \\u0644\\u0647\\u0645 \\u0627\\u0644\\u0635\\u064a\\u0627\\u0645 \\u0641\\u0645 \\u0641\\u0626\\u0647 \\u0645\\u0646 \\u0627\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0627\\u0644\\u0644\\u064a \\u062d\\u0633\\u0628 \\u0627\\u0644\\u062d\\u0627\\u0644\\u0647 \\u0628\\u0639\\u0647\\u0645 \\u062d\\u0633\\u0628 \\u0643\\u0645\\u064a\\u0647 \\u0627\\u0644\\u0627\\u062f\\u0648\\u064a\\u0647 \\u0627\\u0644\\u0644\\u064a \\u064a\\u0627\\u062e\\u0630\\u0648\\u0627 \\u0641\\u064a\\u0647\\u0627 \\u062d\\u0633\\u0628 \\u0645\\u0627\\u0645\\u0627 \\u0642\\u0627\\u0644 \\u062f\\u0643\\u062a\\u0648\\u0631 \\u0628\\u0631\\u062d\\u0648\\u0645\\u0647 \\u062d\\u0627\\u0644\\u0647 \\u0627\\u0644\\u0637\\u0642\\u0633 \\u0627\\u0644\\u0644\\u064a \\u0628\\u062f\\u0627\\u062a \\u062a\\u062a\\u063a\\u064a\\u0631 \\u0634\\u0648\\u064a\\u0647 \\u0627\\u062d\\u0646\\u0627 \\u0633\\u0646 \\u0627\\u0646\\u0627 \\u0643\\u0646\\u062a \\u0631\\u0628\\u0645\\u0627 \\u0645\\u062a\\u0633\\u0627\\u0645\\u062d \\u0634\\u0648\\u064a\\u0647 \\u0645\\u0639 \\u0628\\u0639\\u0636 \\u0627\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0633\\u0646\\u0627 \\u0639\\u0644\\u0649 \\u0639\\u0643\\u0633 \\u0633\\u064a\\u0646 \\u0627\\u0644 \\u0641\\u0627\\u0631\\u0637\\u0647 \\u0628\\u062f\\u064a\\u0646\\u0627 \\u0646\\u0642\\u0631\\u0628 \\u0634\\u0648\\u064a\\u0647 \\u0645\\u0646 \\u0641\\u0635\\u0644 \\u0627\\u0644\\u0628\\u0627\\u0631\\u062f \\u0628\\u062f\\u0627 \\u0646\\u0647\\u0627\\u0631 \\u064a\\u0642\\u0635\\u0631 \\u0634\\u0648\\u064a\\u0647 \\u0648\\u0644\\u0643\\u0646 \\u0646\\u0633\\u0627\\u0648 \\u062a\\u0648\\u0627 \\u0631\\u062c\\u0639\\u0646\\u0627 \\u0644\\u0633\\u062e\\u0627\\u0646\\u0647 \\u0648\\u0646\\u0647\\u0627\\u0631 \\u0627\\u0644\\u0637\\u0648\\u064a\\u0644 \\u0643\\u064a\\u0641 \\u0643\\u064a\\u0641 \\u0644\\u0627\\u0646\\u0647 \\u0627\\u0643\\u062b\\u0631 \\u062d\\u0627\\u062c\\u0647 \\u062a\\u0642\\u0644\\u0642 \\u0647\\u0648 \\u0627\\u0644\\u0637\\u0642\\u0633 \\u0627\\u0644\\u062d\\u0627\\u0631 \\u0648\\u0627\\u0644\\u0646\\u0647\\u0627\\u0631 \\u0627\\u0644\\u0637\\u0648\\u064a\\u0644 \\u0648\\u0627\\u0644\\u0641\\u062a\\u0631\\u0647 \\u0627\\u0644\\u0637\\u0648\\u064a\\u0644\\u0647 \\u0627\\u0644\\u0644\\u064a \\u064a\\u0642\\u0639\\u062f \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0645\\u0627 \\u064a\\u0634\\u0631\\u0628\\u0634 \\u0641\\u064a\\u0647\\u0627 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0648\\u0627\\u0644\\u0627 \\u0645\\u0627 \\u064a\\u062e\\u0634 \\u0641\\u064a\\u0647\\u0627 \\u0627\\u0644\\u0627\\u062f\\u0648\\u064a\\u0647 \\u0627\\u0639\\u0647 \\u0627\\u0644\\u0644\\u064a \\u062d\\u0627\\u062c\\u0647 \\u0645\\u0647\\u0645\\u0647 \\u064a\\u0627\\u0633\\u0631 \\u0641\\u0645 \\u0627\\u062f\\u0648\\u064a\\u0647 \\u062a\\u062a\\u0627\\u062e\\u0630 \\u0639\\u0644\\u0649 \\u0645\\u0631\\u062a\\u064a\\u0646 \\u0648\\u062b\\u0644\\u0627\\u062b \\u0645\\u0631\\u0627\\u062a \\u0641\\u064a \\u0627\\u0644\\u064a\\u0648\\u0645 \\u062f\\u0648\\u0646\\u0643 \\u0627\\u0644\\u0627\\u062e\\u0630 \\u062a\\u0639\\u0647\\u0645 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u0646\\u062a\\u0638\\u0645\\u0647 \\u0639\\u0644\\u0649 \\u0643\\u0627\\u0645\\u0644 \\u0627\\u0644\\u064a\\u0648\\u0645 \\u062d\\u0627\\u062c\\u0647 \\u0645\\u0647\\u0645\\u0647 \\u064a\\u0627\\u0633\\u0631 \\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0644\\u0644\\u0645\\u0631\\u0636\\u0649 \\u0647\\u0630\\u0648\\u0645\\u0627\",\n          \"\\u0644\\u0648\\u0636\\u0639\\u064a\\u0647 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0628\\u062e\\u0644\\u0627\\u0641 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0647\\u0648 \\u0627\\u0643\\u062b\\u0631 \\u062d\\u0627\\u062c\\u0647 \\u0646\\u0628\\u0647\\u0648\\u0627 \\u0645\\u0646\\u0647\\u0627 \\u0648 \\u0646\\u0642\\u0648\\u0644\\u0648\\u0627 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u062a\\u0628\\u0639\\u062f \\u0639\\u0644\\u064a\\u0647\\u0627 \\u0647\\u064a \\u0627\\u0644\\u0628\\u0631\\u0648\\u062a\\u064a\\u0646\\u0627\\u062a \\u0627\\u0644\\u062d\\u064a\\u0648\\u0627\\u0646\\u064a\\u0647 \\u0648\\u062e\\u0627\\u0635\\u0647 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0627\\u0644\\u062d\\u0645\\u0631\\u0627\\u0621 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0645\\u0639\\u0646\\u0627\\u062a\\u0647\\u0627 \\u0627\\u0643\\u062b\\u0631 \\u062d\\u0627\\u062c\\u0627\\u062a \\u0627\\u0644\\u0644\\u064a \\u062a\\u0646\\u062c\\u0645 \\u062a\\u0627\\u062b\\u0631 \\u0633\\u0644\\u0628\\u064a\\u0627 \\u0639\\u0644\\u0649 \\u0648\\u0638\\u0627\\u0626\\u0641 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0646\\u0642\\u0648\\u0644 \\u0644\\u0647\\u0645 \\u0627\\u062d\\u0646\\u0627 \\u064a\\u0646\\u0642\\u0635\\u0648\\u0627 \\u0645\\u0646 \\u0627\\u0644\\u0628\\u0631\\u0648\\u062a\\u064a\\u0646\\u0627\\u062a \\u0627\\u0644\\u062d\\u064a\\u0648\\u0627\\u0646\\u064a\\u0647 \\u0648\\u062e\\u0627\\u0635\\u0647 \\u0645\\u0646 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0627\\u0644\\u062d\\u0645\\u0631\\u0627\\u0621 \\u064a\\u0627\\u062e\\u0630\\u0648\\u0627 \\u064a\\u0646 \\u0641\\u0636\\u0644\\u0648\\u0627 \\u0627\\u0644\\u062e\\u0636\\u0631 \\u0627\\u0630\\u0627 \\u0643\\u0627\\u0646 \\u0648\\u0636\\u0639\\u064a\\u0647 \\u0627\\u0644\\u0628\\u0648\\u062a\\u0627\\u0633\\u064a\\u0648\\u0645 \\u0641\\u064a \\u0627\\u0644\\u0628\\u062f\\u0646 \\u062a\\u0633\\u0645\\u062d \\u0648\\u0644\\u0627 \\u0627\\u0644\\u0627\\u0643\\u0644 \\u0645\\u062a\\u0648\\u0627\\u0632\\u0646 \\u0648\\u0644\\u0643\\u0646 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0646\\u0628\\u062a\\u0639\\u062f \\u0639\\u0644\\u064a\\u0647\\u0627 \\u0627\\u0643\\u062b\\u0631 \\u0645\\u0627 \\u064a\\u0645\\u0643\\u0646 \\u0646\\u0627\\u062e\\u0630 \\u0643\\u0645\\u064a\\u0627\\u062a \\u0627\\u0644\\u062d\\u0627\\u062c\\u064a\\u0627\\u062a \\u0627\\u0644\\u0628\\u062f\\u0646 \\u0643\\u0645\\u064a\\u0647 \\u0645\\u062d\\u062f\\u0647 \\u062d\\u0633\\u0628 \\u0627\\u0644\\u0648\\u0632\\u0646 \\u0627\\u0639 \\u0627\\u0644\\u0645\\u0631\\u064a\\u0636 \\u0646\\u0641\\u0636\\u0644\\u0647 \\u0645\\u062b\\u0644\\u0627 \\u0627\\u0644\\u0627\\u0633\\u0645\\u0627\\u0643 \\u0648\\u0644\\u0627 \\u0627\\u0628\\u064a\\u0636 \\u0627\\u0644\\u0628\\u064a\\u0636 \\u0648\\u0644\\u0627 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0627\\u0644\\u0628\\u064a\\u0636\\u0627\\u0621 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u0644\\u062d\\u0648\\u0645 \\u0627\\u0644\\u062d\\u0645\\u0631\\u0627\\u0621 \\u0648\\u062f\\u064a\\u0645\\u0627 \\u0646\\u0642\\u0639\\u062f \\u0628\\u0643\\u0645\\u064a\\u0627\\u062a \\u0645\\u0639\\u062a\\u062f\\u0644\\u0647\",\n          \"\\u0628\\u0627\\u0644\\u0646\\u0633\\u0628\\u0647 \\u0627\\u0644\\u0644\\u064a \\u0632\\u0627\\u0631\\u0639\\u064a\\u0646 \\u0627\\u0644\\u0643\\u0644\\u0627 \\u0627\\u0644\\u0644\\u064a \\u0632\\u0627\\u0631\\u0639\\u064a\\u0646 \\u0627\\u0644\\u0643\\u0644\\u0627\\u0621 \\u062f\\u0648\\u0643\\u0645 \\u0647\\u064a \\u0639\\u0628\\u0627\\u0631\\u0647 \\u0643\\u0644\\u0648\\u0647 \\u0648\\u0627\\u062d\\u062f\\u0647 \\u062a\\u0634\\u062a\\u063a\\u0644 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0646\\u0627\\u0633 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u0642\\u0648\\u0644 \\u0644\\u0647\\u0645 \\u064a\\u0644\\u0632\\u0645\\u0643\\u0645 \\u062a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0645\\u0627 \\u0643\\u0645\\u064a\\u0647 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u0645\\u0646\\u0647\\u0627\\u0631 \\u0646\\u0632\\u0631\\u0639 \\u0648\\u0627\\u062d\\u0646\\u0627 \\u0646\\u0628\\u062f\\u0627 \\u0646\\u0648\\u0635 \\u064a\\u0648\\u0647\\u0645 \\u0639\\u0627\\u062f\\u0647 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0628\\u064a\\u0646 \\u0644\\u062a\\u0631\\u064a\\u0646 \\u0648\\u062b\\u0644\\u0627 \\u0644\\u062a\\u0631\\u0627\\u062a \\u0641\\u064a \\u0627\\u0644\\u064a\\u0648\\u0645 \\u0645\\u0639\\u0646\\u0627\\u062a\\u0647\\u0627 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u062a\\u0641\\u0631\\u0642\\u0647 \\u0639\\u0644\\u0649 \\u0627\\u0644\\u064a\\u0648\\u0645 \\u0645\\u0634 \\u064a\\u0642\\u0639 \\u0627\\u0644\\u062a\\u063a\\u0630\\u064a\\u0647 \\u0627\\u0639 \\u0627\\u0644\\u062c\\u0633\\u0645 \\u0643\\u0644\\u0647 \\u064a\\u0642\\u0639\\u062f \\u0645\\u0627 \\u064a\\u0642\\u0639\\u0634 \\u0641\\u064a \\u062d\\u0627\\u0644\\u0647 \\u0627\\u0644\\u062c\\u0641\\u0627\\u0641 \\u0644\\u0627\\u0646 \\u062d\\u0627\\u0644\\u0647 \\u0627\\u0644\\u062c\\u0641\\u0627\\u0641 \\u0643\\u0644\\u0648\\u0647 \\u0648\\u062f\\u0647 \\u062a\\u0627\\u062b\\u0631 \\u0628\\u0635\\u0641\\u0647 \\u0643\\u0628\\u064a\\u0631\\u0647 \\u0645\\u0634 \\u0643\\u0645\\u0627 \\u064a\\u0628\\u062f\\u0627\\u0648 \\u0643\\u0644\\u064a\\u062a\\u064a\\u0646 \\u0645\\u0639 \\u0628\\u0639\\u0636\\u0647\\u0645 \\u0648\\u0644\\u0630\\u0627 \\u062f\\u0648\\u0646\\u0643 \\u0627\\u0644\\u0644\\u064a \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0632\\u0631\\u0639 \\u0627\\u0644\\u0643\\u0644\\u0627\\u0621 \\u0627\\u062d\\u0646\\u0627 \\u0646\\u0648\\u0635\\u0644\\u0647\\u0645 \\u0628\\u0627\\u0634 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u0627\\u0644\\u0645\\u0627\\u0621 \\u0628\\u0635\\u0641\\u0647 \\u0645\\u0633\\u062a\\u0645\\u0631\\u0647 \\u0648\\u0639\\u0644\\u0649 \\u0643\\u0627\\u0645\\u0644 \\u0627\\u0644\\u064a\\u0648\\u0645 \\u062a\\u0641\\u0631\\u0642 \\u0639\\u0644\\u0649 \\u0643\\u0627\\u0645\\u0644 \\u0627\\u0644\\u064a\\u0648\\u0645 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u0627\\u0644\\u0644\\u064a \\u062a\\u0639\\u0645\\u0644 \\u0641\\u064a \\u0627\\u0644\\u063a\\u0633\\u064a\\u0644 \\u0628\\u0648\\u0636\\u0639\\u064a\\u0647 \\u0627\\u062e\\u0631\\u0649 \\u0646\\u0627\\u0633 \\u062a\\u0639\\u0645\\u0644 \\u0641\\u064a \\u0627\\u0644\\u063a\\u0633\\u064a\\u0644 \\u0648\\u0644\\u0627 \\u0639\\u0646\\u062f\\u0647\\u0645 \\u0642\\u0635\\u0648\\u0631 \\u0643\\u0644\\u0627\\u0621 \\u0645\\u062a\\u0642\\u062f\\u0645 \\u0643\\u062b\\u064a\\u0631 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0646\\u0627\\u0633 \\u0641\\u064a \\u0627\\u0644\\u0639\\u0627\\u062f\\u0647 \\u064a\\u0646\\u0642\\u0635 \\u0643\\u0645\\u064a\\u0647 \\u0627\\u0644\\u0628\\u0648\\u0644\\u0647 \\u0628\\u0639\\u0647\\u0645 \\u0648\\u0627\\u0644\\u0644\\u064a \\u0641\\u064a \\u0627\\u0644\\u063a\\u0633\\u064a\\u0644 \\u0641\\u064a \\u0627\\u0644\\u0639\\u0627\\u062f\\u0647 \\u0645\\u0627 \\u0639\\u0646\\u062f\\u0647\\u0645\\u0634 \\u062a\\u0628\\u0648\\u0644 \\u062a\\u0642\\u0631\\u064a\\u0628\\u0627 \\u062a\\u0645\\u0627\\u0645\\u0627 \\u0648\\u0647\\u062f\\u0648 \\u0646\\u0627\\u0633 \\u0645\\u0627 \\u064a\\u0632\\u0648 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u064a\\u0627\\u0633\\u0631 \\u0645\\u0627\\u0621 \\u062d\\u062a\\u0649 \\u0648\\u0644\\u0648 \\u0637\\u0642\\u0633 \\u062d\\u0627\\u0631 \\u0627\\u064a\\u0647 \\u0645\\u0627\\u0632\\u0645 \\u064a\\u0634 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0627 \\u064a\\u0627\\u0633\\u0631 \\u0645\\u0627\\u0621 \\u0644\\u0627\\u0646\\u0647 \\u062d\\u062a\\u0649 \\u0648\\u0644\\u0648 \\u062d\\u062a\\u0649 \\u0627\\u0644\\u062a\\u0639\\u0631\\u0642 \\u0628\\u0639\\u0647\\u0645 \\u064a\\u0646\\u0642\\u0635 \\u0628\\u0627\\u0646\\u0647 \\u0627\\u0644\\u062c\\u0644\\u062f \\u0628\\u0639\\u0647\\u0645 \\u064a\\u0642\\u0639 \\u0641\\u064a\\u0647\\u0627 \\u0643 \\u0646\\u0648\\u0639 \\u0645\\u0646 \\u0627\\u0644\\u062c\\u0641\\u0627\\u0641 \\u0648\\u0641\\u064a \\u0639\\u0627\\u062f\\u0647 \\u062d\\u062a\\u0649 \\u0627\\u0644\\u062a\\u0639\\u0631\\u0642 \\u0645\\u062a\\u0627\\u0639\\u0647\\u0645 \\u0628\\u064a\\u0628\\u062f\\u0627 \\u0627\\u0642\\u0644 \\u0645\\u0646 \\u0627\\u0644\\u0627\\u0646\\u0633\\u0627\\u0646 \\u0627\\u0644\\u0639\\u0627\\u062f\\u064a \\u0648\\u0644\\u0630\\u0627 \\u0627\\u0644\\u0646\\u0627\\u0633 \\u062a\\u0639\\u0645 \\u0641\\u064a \\u0627\\u0644\\u063a\\u0633\\u064a\\u0644 \\u0627\\u0644\\u062f\\u064a\\u0627\\u0644\\u064a\\u0632 \\u0647\\u0630\\u0648\\u0645\\u0627 \\u0646\\u0627\\u0633 \\u0646\\u0648\\u0635\\u0648 \\u0645\\u0634 \\u0645\\u0627 \\u064a\\u0634\\u0631\\u0628\\u0648 \\u064a\\u0627\\u0633\\u0631 \\u0645\\u0627\\u0621 \\u062d\\u062a\\u0649 \\u0648\\u0644\\u0648 \\u0637\\u0642\\u0633 \\u062d\\u0627\\u0631 \\u0644\\u0627\\u0646\\u0647 \\u0627\\u0644\\u0645\\u0627\\u0644 \\u064a\\u0634\\u0631\\u0628\\u0648\\u0647 \\u064a\\u062a\\u062e\\u0632\\u0646 \\u0641\\u064a \\u0627\\u0644\\u0628\\u062f\\u0646 \\u0648\\u0627\\u0644\\u0643\\u0644\\u0627 \\u0628\\u0639\\u0647\\u0645 \\u0645\\u0627\\u0647\\u064a\\u0634 \\u0642\\u0627\\u062f\\u0631\\u0647 \\u0639\\u0644\\u0649 \\u0644\\u0644\\u0627\\u062e\\u0631\\u0627\\u062c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHjTSWXbP28H",
        "outputId": "fef6c0eb-bf07-47cd-f428-a2ee86fb29ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df_final_global.iterrows():\n",
        "    question = row['Question']\n",
        "\n",
        "    tokenized_question = tokenizer(question, return_tensors=\"pt\")\n",
        "\n",
        "    # Calculer le nombre de tokens utilisés par la question\n",
        "    num_tokens_question = len(tokenized_question['input_ids'][0])\n",
        "\n",
        "    # Déterminer combien de tokens sont disponibles pour la réponse\n",
        "    max_tokens = 512\n",
        "    tokens_restants_pour_reponse = max_tokens - num_tokens_question\n",
        "\n",
        "    print(f\"Nombre de tokens pour la question : {num_tokens_question}\")\n",
        "    print(f\"Tokens disponibles pour la réponse : {tokens_restants_pour_reponse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmIs0XAI7-J7",
        "outputId": "260d2245-fa6e-46a8-ceb4-7e64f17ce5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de tokens pour la question : 34\n",
            "Tokens disponibles pour la réponse : 478\n",
            "Nombre de tokens pour la question : 41\n",
            "Tokens disponibles pour la réponse : 471\n",
            "Nombre de tokens pour la question : 34\n",
            "Tokens disponibles pour la réponse : 478\n",
            "Nombre de tokens pour la question : 58\n",
            "Tokens disponibles pour la réponse : 454\n",
            "Nombre de tokens pour la question : 18\n",
            "Tokens disponibles pour la réponse : 494\n",
            "Nombre de tokens pour la question : 39\n",
            "Tokens disponibles pour la réponse : 473\n",
            "Nombre de tokens pour la question : 55\n",
            "Tokens disponibles pour la réponse : 457\n",
            "Nombre de tokens pour la question : 54\n",
            "Tokens disponibles pour la réponse : 458\n",
            "Nombre de tokens pour la question : 34\n",
            "Tokens disponibles pour la réponse : 478\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 14\n",
            "Tokens disponibles pour la réponse : 498\n",
            "Nombre de tokens pour la question : 63\n",
            "Tokens disponibles pour la réponse : 449\n",
            "Nombre de tokens pour la question : 37\n",
            "Tokens disponibles pour la réponse : 475\n",
            "Nombre de tokens pour la question : 58\n",
            "Tokens disponibles pour la réponse : 454\n",
            "Nombre de tokens pour la question : 12\n",
            "Tokens disponibles pour la réponse : 500\n",
            "Nombre de tokens pour la question : 44\n",
            "Tokens disponibles pour la réponse : 468\n",
            "Nombre de tokens pour la question : 40\n",
            "Tokens disponibles pour la réponse : 472\n",
            "Nombre de tokens pour la question : 23\n",
            "Tokens disponibles pour la réponse : 489\n",
            "Nombre de tokens pour la question : 63\n",
            "Tokens disponibles pour la réponse : 449\n",
            "Nombre de tokens pour la question : 47\n",
            "Tokens disponibles pour la réponse : 465\n",
            "Nombre de tokens pour la question : 40\n",
            "Tokens disponibles pour la réponse : 472\n",
            "Nombre de tokens pour la question : 143\n",
            "Tokens disponibles pour la réponse : 369\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 40\n",
            "Tokens disponibles pour la réponse : 472\n",
            "Nombre de tokens pour la question : 26\n",
            "Tokens disponibles pour la réponse : 486\n",
            "Nombre de tokens pour la question : 31\n",
            "Tokens disponibles pour la réponse : 481\n",
            "Nombre de tokens pour la question : 18\n",
            "Tokens disponibles pour la réponse : 494\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 16\n",
            "Tokens disponibles pour la réponse : 496\n",
            "Nombre de tokens pour la question : 31\n",
            "Tokens disponibles pour la réponse : 481\n",
            "Nombre de tokens pour la question : 84\n",
            "Tokens disponibles pour la réponse : 428\n",
            "Nombre de tokens pour la question : 23\n",
            "Tokens disponibles pour la réponse : 489\n",
            "Nombre de tokens pour la question : 27\n",
            "Tokens disponibles pour la réponse : 485\n",
            "Nombre de tokens pour la question : 39\n",
            "Tokens disponibles pour la réponse : 473\n",
            "Nombre de tokens pour la question : 49\n",
            "Tokens disponibles pour la réponse : 463\n",
            "Nombre de tokens pour la question : 15\n",
            "Tokens disponibles pour la réponse : 497\n",
            "Nombre de tokens pour la question : 16\n",
            "Tokens disponibles pour la réponse : 496\n",
            "Nombre de tokens pour la question : 10\n",
            "Tokens disponibles pour la réponse : 502\n",
            "Nombre de tokens pour la question : 32\n",
            "Tokens disponibles pour la réponse : 480\n",
            "Nombre de tokens pour la question : 12\n",
            "Tokens disponibles pour la réponse : 500\n",
            "Nombre de tokens pour la question : 32\n",
            "Tokens disponibles pour la réponse : 480\n",
            "Nombre de tokens pour la question : 20\n",
            "Tokens disponibles pour la réponse : 492\n",
            "Nombre de tokens pour la question : 11\n",
            "Tokens disponibles pour la réponse : 501\n",
            "Nombre de tokens pour la question : 25\n",
            "Tokens disponibles pour la réponse : 487\n",
            "Nombre de tokens pour la question : 16\n",
            "Tokens disponibles pour la réponse : 496\n",
            "Nombre de tokens pour la question : 26\n",
            "Tokens disponibles pour la réponse : 486\n",
            "Nombre de tokens pour la question : 48\n",
            "Tokens disponibles pour la réponse : 464\n",
            "Nombre de tokens pour la question : 8\n",
            "Tokens disponibles pour la réponse : 504\n",
            "Nombre de tokens pour la question : 35\n",
            "Tokens disponibles pour la réponse : 477\n",
            "Nombre de tokens pour la question : 23\n",
            "Tokens disponibles pour la réponse : 489\n",
            "Nombre de tokens pour la question : 82\n",
            "Tokens disponibles pour la réponse : 430\n",
            "Nombre de tokens pour la question : 37\n",
            "Tokens disponibles pour la réponse : 475\n",
            "Nombre de tokens pour la question : 82\n",
            "Tokens disponibles pour la réponse : 430\n",
            "Nombre de tokens pour la question : 31\n",
            "Tokens disponibles pour la réponse : 481\n",
            "Nombre de tokens pour la question : 40\n",
            "Tokens disponibles pour la réponse : 472\n",
            "Nombre de tokens pour la question : 52\n",
            "Tokens disponibles pour la réponse : 460\n",
            "Nombre de tokens pour la question : 47\n",
            "Tokens disponibles pour la réponse : 465\n",
            "Nombre de tokens pour la question : 54\n",
            "Tokens disponibles pour la réponse : 458\n",
            "Nombre de tokens pour la question : 8\n",
            "Tokens disponibles pour la réponse : 504\n",
            "Nombre de tokens pour la question : 20\n",
            "Tokens disponibles pour la réponse : 492\n",
            "Nombre de tokens pour la question : 73\n",
            "Tokens disponibles pour la réponse : 439\n",
            "Nombre de tokens pour la question : 9\n",
            "Tokens disponibles pour la réponse : 503\n",
            "Nombre de tokens pour la question : 14\n",
            "Tokens disponibles pour la réponse : 498\n",
            "Nombre de tokens pour la question : 40\n",
            "Tokens disponibles pour la réponse : 472\n",
            "Nombre de tokens pour la question : 19\n",
            "Tokens disponibles pour la réponse : 493\n",
            "Nombre de tokens pour la question : 34\n",
            "Tokens disponibles pour la réponse : 478\n",
            "Nombre de tokens pour la question : 10\n",
            "Tokens disponibles pour la réponse : 502\n",
            "Nombre de tokens pour la question : 24\n",
            "Tokens disponibles pour la réponse : 488\n",
            "Nombre de tokens pour la question : 190\n",
            "Tokens disponibles pour la réponse : 322\n",
            "Nombre de tokens pour la question : 60\n",
            "Tokens disponibles pour la réponse : 452\n",
            "Nombre de tokens pour la question : 19\n",
            "Tokens disponibles pour la réponse : 493\n",
            "Nombre de tokens pour la question : 45\n",
            "Tokens disponibles pour la réponse : 467\n",
            "Nombre de tokens pour la question : 58\n",
            "Tokens disponibles pour la réponse : 454\n",
            "Nombre de tokens pour la question : 57\n",
            "Tokens disponibles pour la réponse : 455\n",
            "Nombre de tokens pour la question : 35\n",
            "Tokens disponibles pour la réponse : 477\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 12\n",
            "Tokens disponibles pour la réponse : 500\n",
            "Nombre de tokens pour la question : 19\n",
            "Tokens disponibles pour la réponse : 493\n",
            "Nombre de tokens pour la question : 16\n",
            "Tokens disponibles pour la réponse : 496\n",
            "Nombre de tokens pour la question : 886\n",
            "Tokens disponibles pour la réponse : -374\n",
            "Nombre de tokens pour la question : 61\n",
            "Tokens disponibles pour la réponse : 451\n",
            "Nombre de tokens pour la question : 19\n",
            "Tokens disponibles pour la réponse : 493\n",
            "Nombre de tokens pour la question : 37\n",
            "Tokens disponibles pour la réponse : 475\n",
            "Nombre de tokens pour la question : 13\n",
            "Tokens disponibles pour la réponse : 499\n",
            "Nombre de tokens pour la question : 19\n",
            "Tokens disponibles pour la réponse : 493\n",
            "Nombre de tokens pour la question : 33\n",
            "Tokens disponibles pour la réponse : 479\n",
            "Nombre de tokens pour la question : 28\n",
            "Tokens disponibles pour la réponse : 484\n",
            "Nombre de tokens pour la question : 33\n",
            "Tokens disponibles pour la réponse : 479\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 11\n",
            "Tokens disponibles pour la réponse : 501\n",
            "Nombre de tokens pour la question : 61\n",
            "Tokens disponibles pour la réponse : 451\n",
            "Nombre de tokens pour la question : 42\n",
            "Tokens disponibles pour la réponse : 470\n",
            "Nombre de tokens pour la question : 20\n",
            "Tokens disponibles pour la réponse : 492\n",
            "Nombre de tokens pour la question : 13\n",
            "Tokens disponibles pour la réponse : 499\n",
            "Nombre de tokens pour la question : 7\n",
            "Tokens disponibles pour la réponse : 505\n",
            "Nombre de tokens pour la question : 16\n",
            "Tokens disponibles pour la réponse : 496\n",
            "Nombre de tokens pour la question : 6\n",
            "Tokens disponibles pour la réponse : 506\n",
            "Nombre de tokens pour la question : 49\n",
            "Tokens disponibles pour la réponse : 463\n",
            "Nombre de tokens pour la question : 9\n",
            "Tokens disponibles pour la réponse : 503\n",
            "Nombre de tokens pour la question : 9\n",
            "Tokens disponibles pour la réponse : 503\n",
            "Nombre de tokens pour la question : 829\n",
            "Tokens disponibles pour la réponse : -317\n",
            "Nombre de tokens pour la question : 13\n",
            "Tokens disponibles pour la réponse : 499\n",
            "Nombre de tokens pour la question : 24\n",
            "Tokens disponibles pour la réponse : 488\n",
            "Nombre de tokens pour la question : 32\n",
            "Tokens disponibles pour la réponse : 480\n",
            "Nombre de tokens pour la question : 13\n",
            "Tokens disponibles pour la réponse : 499\n",
            "Nombre de tokens pour la question : 9\n",
            "Tokens disponibles pour la réponse : 503\n",
            "Nombre de tokens pour la question : 28\n",
            "Tokens disponibles pour la réponse : 484\n",
            "Nombre de tokens pour la question : 30\n",
            "Tokens disponibles pour la réponse : 482\n",
            "Nombre de tokens pour la question : 38\n",
            "Tokens disponibles pour la réponse : 474\n",
            "Nombre de tokens pour la question : 17\n",
            "Tokens disponibles pour la réponse : 495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segmenter_reponse(reponse, segment_size=100):\n",
        "    tokens = reponse.split()  # Diviser la réponse en mots\n",
        "    return [' '.join(tokens[i:i+segment_size]) for i in range(0, len(tokens), segment_size)]\n",
        "\n",
        "# Exemple d'application de la fonction sur une colonne de réponse\n",
        "df_final_global['Segments_Reponse'] = df_final_global['Réponse'].apply(segmenter_reponse)"
      ],
      "metadata": {
        "id": "zLga2XMy7-Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xlwu2IO7-E7",
        "outputId": "900fec60-01f5-45d6-8191-7fba2c862b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Charger le modèle Sentence-BERT pour l'analyse sémantique\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Fonction pour calculer la similarité sémantique\n",
        "def trier_segments_par_pertinence(row):\n",
        "    question_embedding = model.encode(row['Question'], convert_to_tensor=True)\n",
        "    segments = row['Segments_Reponse']\n",
        "    segment_embeddings = model.encode(segments, convert_to_tensor=True)\n",
        "\n",
        "    # Calculer la similarité cosinus entre la question et chaque segment de la réponse\n",
        "    similarities = util.pytorch_cos_sim(question_embedding, segment_embeddings)\n",
        "\n",
        "    # Trier les segments par ordre de pertinence\n",
        "    segments_tries = [x for _, x in sorted(zip(similarities, segments), reverse=True)]\n",
        "    return segments_tries\n",
        "\n",
        "# Appliquer l'analyse sémantique sur le DataFrame\n",
        "df_final_global['Segments_Triés'] = df_final_global.apply(trier_segments_par_pertinence, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxL-ku5v7907",
        "outputId": "7c290b77-8af6-45fa-9358-a6129b0a11d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construire_sequence_finale(row):\n",
        "    tokenized_question = tokenizer(row['Question'], return_tensors=\"pt\")\n",
        "    tokens_utilises = len(tokenized_question['input_ids'][0])\n",
        "\n",
        "    sequence_finale = row['Question']\n",
        "\n",
        "    # Ajouter les segments de la réponse triés par pertinence, un par un\n",
        "    for segment in row['Segments_Triés']:\n",
        "        tokenized_segment = tokenizer(segment, return_tensors=\"pt\")\n",
        "        tokens_utilises += len(tokenized_segment['input_ids'][0])\n",
        "\n",
        "        if tokens_utilises <= 512:  # S'assurer de ne pas dépasser 512 tokens\n",
        "            sequence_finale += \" \" + segment\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return sequence_finale\n",
        "\n",
        "# Appliquer la construction de séquence finale sur le DataFrame\n",
        "df_final_global['Sequence_Finale'] = df_final_global.apply(construire_sequence_finale, axis=1)\n"
      ],
      "metadata": {
        "id": "JSsGlhrk79yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generer_positions_start_end(row):\n",
        "    # Tokeniser la question et la séquence finale\n",
        "    tokenized_question = tokenizer(row['Question'], return_tensors=\"pt\", truncation=True)\n",
        "    tokenized_sequence = tokenizer(row['Sequence_Finale'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    # La position \"start\" est juste après la question\n",
        "    len_question = len(tokenized_question['input_ids'][0])\n",
        "    start_position = len_question  # Le premier token de la réponse est après la question\n",
        "\n",
        "    # La position \"end\" est à la fin de la réponse dans la séquence finale\n",
        "    len_sequence = len(tokenized_sequence['input_ids'][0])\n",
        "    end_position = len_sequence - 1  # Dernier token de la séquence finale\n",
        "\n",
        "    return start_position, end_position\n",
        "\n",
        "# Appliquer la fonction pour générer les positions \"start\" et \"end\" sur chaque ligne du DataFrame\n",
        "df_final_global['start_position'], df_final_global['end_position'] = zip(*df_final_global.apply(generer_positions_start_end, axis=1))\n",
        "\n",
        "# Vérifier les positions de début et de fin\n",
        "print(df_final_global[['Sequence_Finale', 'start_position', 'end_position']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KvFyosN79pL",
        "outputId": "48ae257e-68b3-49bb-b30c-5496646a94e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Sequence_Finale  start_position  \\\n",
            "0    كيفاش هالارتفاع المفرد في درجات الحراره وخاصه ...              34   \n",
            "1    الفئات الناس اللي عندها الحساء يلزم ترد بالها ...              41   \n",
            "2    العلامات دكتور والاعراض ماما قاعدين نشوف العلا...              34   \n",
            "3    الفئات الاكثر عرضه دكتور باي الجفاف هذا كيفاش ...              58   \n",
            "4    بالنسبه ل عندهم زرع دكتور اللي سناو في زرع ما ...              18   \n",
            "..                                                 ...             ...   \n",
            "105  شنو هو العلاج اع مرض بهج علاج في مرض بهجت فيه ...               9   \n",
            "106  نرجع للنصائح الى جانب التشخيص الدقيق والحين وا...              28   \n",
            "107  علاش اخترتوا ه النظام الغذائي هذايا وشنو منفعت...              30   \n",
            "108  علاش حتى نحكي على مؤتمر متوسطي للطب الباطني وب...              38   \n",
            "109  حوصله مرض بهجه شنو تنجم تزيد تحكينا على النصائ...              17   \n",
            "\n",
            "     end_position  \n",
            "0             189  \n",
            "1             184  \n",
            "2             190  \n",
            "3             201  \n",
            "4             181  \n",
            "..            ...  \n",
            "105           145  \n",
            "106           176  \n",
            "107           188  \n",
            "108           200  \n",
            "109           161  \n",
            "\n",
            "[110 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokeniser toutes les séquences finales et vérifier la longueur après tokenisation\n",
        "def verifier_longueur_apres_tokenisation(sequence):\n",
        "    tokenized_sequence = tokenizer(sequence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    return len(tokenized_sequence['input_ids'][0])\n",
        "\n",
        "# Appliquer la vérification pour toutes les séquences dans le DataFrame\n",
        "df_final_global['longueur_tokens'] = df_final_global['Sequence_Finale'].apply(verifier_longueur_apres_tokenisation)\n",
        "\n",
        "# Vérifier si certaines séquences dépassent la longueur de 512 tokens\n",
        "sequences_trop_longues = df_final_global[df_final_global['longueur_tokens'] > 512]\n",
        "\n",
        "if not sequences_trop_longues.empty:\n",
        "    print(\"Attention : certaines séquences dépassent la limite de 512 tokens !\")\n",
        "    print(sequences_trop_longues[['Sequence_Finale', 'longueur_tokens']])\n",
        "else:\n",
        "    print(\"Toutes les séquences respectent la limite de 512 tokens.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUAB366z79mb",
        "outputId": "06477abd-60e4-4dd2-da16-a25c31ac1b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toutes les séquences respectent la limite de 512 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokeniser les séquences avec une longueur maximale de 512 tokens\n",
        "inputs = tokenizer(\n",
        "    df_final_global['Sequence_Finale'].tolist(),  # Les séquences d'entrée\n",
        "    max_length=512,  # Fixer la longueur maximale à 512 tokens\n",
        "    padding=\"max_length\",  # Remplir les séquences plus courtes avec des tokens de padding\n",
        "    truncation=True,  # Tronquer les séquences plus longues\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "_OI_X6Fy9-8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier la longueur des séquences après tokenisation\n",
        "longueurs_sequences = [len(seq) for seq in inputs['input_ids']]\n",
        "print(f\"Longueur maximale après tokenisation et tronquage : {max(longueurs_sequences)}\")  # Devrait être 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXdVlYkB-CCz",
        "outputId": "08fb9771-777e-408e-8563-2cdfc682294b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longueur maximale après tokenisation et tronquage : 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Diviser les données en 80% pour l'entraînement et 20% pour la validation\n",
        "train_df, val_df = train_test_split(df_final_global, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vérifier la taille des jeux de données\n",
        "print(f\"Taille du jeu d'entraînement : {len(train_df)}\")\n",
        "print(f\"Taille du jeu de validation : {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M32kigfw_eGN",
        "outputId": "c9a8755a-b5a8-4485-9a84-ce470f05581f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du jeu d'entraînement : 88\n",
            "Taille du jeu de validation : 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assurez-vous que la fonction generer_labels_binaires est définie\n",
        "def generer_labels_binaires(start_position, end_position, seq_length):\n",
        "    labels = [0] * seq_length  # Par défaut, tous les tokens sont marqués comme non pertinents (0)\n",
        "    for i in range(start_position, end_position + 1):\n",
        "        if i < seq_length:  # Vérifiez que la position est dans la séquence\n",
        "            labels[i] = 1  # Marquer comme faisant partie de la réponse\n",
        "    return labels"
      ],
      "metadata": {
        "id": "2ngxviLh_fYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import the torch module\n",
        "# Initialisation des listes pour l'entraînement\n",
        "train_input_ids = []\n",
        "train_attention_mask = []\n",
        "train_labels = []\n",
        "\n",
        "# Tokeniser chaque séquence finale et générer les labels binaires pour l'entraînement\n",
        "for index, row in train_df.iterrows():\n",
        "    inputs = tokenizer(row['Sequence_Finale'], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    seq_length = len(inputs['input_ids'][0])\n",
        "\n",
        "    # Générer les labels binaires\n",
        "    start_position = row['start_position']\n",
        "    end_position = row['end_position']\n",
        "    labels = generer_labels_binaires(start_position, end_position, seq_length)\n",
        "\n",
        "    # Stocker les résultats\n",
        "    train_input_ids.append(inputs['input_ids'])\n",
        "    train_attention_mask.append(inputs['attention_mask'])\n",
        "    train_labels.append(labels)\n",
        "\n",
        "# Transformer en tensors\n",
        "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
        "train_attention_mask = torch.cat(train_attention_mask, dim=0)\n",
        "train_labels = torch.tensor(train_labels)"
      ],
      "metadata": {
        "id": "ISVPUzVD_fWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation des listes pour la validation\n",
        "val_input_ids = []\n",
        "val_attention_mask = []\n",
        "val_labels = []\n",
        "\n",
        "# Tokeniser chaque séquence finale et générer les labels binaires pour la validation\n",
        "for index, row in val_df.iterrows():\n",
        "    inputs = tokenizer(row['Sequence_Finale'], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    seq_length = len(inputs['input_ids'][0])\n",
        "\n",
        "    # Générer les labels binaires\n",
        "    start_position = row['start_position']\n",
        "    end_position = row['end_position']\n",
        "    labels = generer_labels_binaires(start_position, end_position, seq_length)\n",
        "\n",
        "    # Stocker les résultats\n",
        "    val_input_ids.append(inputs['input_ids'])\n",
        "    val_attention_mask.append(inputs['attention_mask'])\n",
        "    val_labels.append(labels)\n",
        "\n",
        "# Transformer en tensors\n",
        "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
        "val_attention_mask = torch.cat(val_attention_mask, dim=0)\n",
        "val_labels = torch.tensor(val_labels)\n"
      ],
      "metadata": {
        "id": "a_9nLHQl_fT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Charger le modèle pré-entraîné pour le fine-tuning\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "device = torch.device('cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85aHqAbQ-bGT",
        "outputId": "7c15116a-f0b6-49e0-a0ca-4b8bba066dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=64000, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Créer un dataset PyTorch pour l'entraînement\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "\n",
        "# Créer un DataLoader pour l'entraînement\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "C_QGmXLC_fRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un dataset PyTorch pour la validation\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
        "\n",
        "# Créer un DataLoader pour la validation\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "B7R_7xIe_eDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier la taille des input_ids, attention_mask et labels\n",
        "print(f\"Taille des input_ids : {train_input_ids.shape}\")\n",
        "print(f\"Taille des attention_mask : {train_attention_mask.shape}\")\n",
        "print(f\"Taille des labels : {train_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNjNh4Ut_xH7",
        "outputId": "981cba83-31e1-446b-d396-b2dbf0a00cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille des input_ids : torch.Size([88, 512])\n",
            "Taille des attention_mask : torch.Size([88, 512])\n",
            "Taille des labels : torch.Size([88, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.bert.encoder.layer[0].attention.self.dropout.p = 0.3  # Augmenter à 30%"
      ],
      "metadata": {
        "id": "phlmeF9Zs_w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, optimizer, device):\n",
        "    model.train()  # Passer en mode entraînement\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].long().to(device)  # Convertir les labels en LongTensor\n",
        "\n",
        "        optimizer.zero_grad()  # Réinitialiser les gradients\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss  # Calculer la perte\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Mise à jour des poids\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Perte d'entraînement moyenne : {avg_train_loss}\")\n",
        "    return avg_train_loss\n"
      ],
      "metadata": {
        "id": "2Uf7UDBq8YS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the import statement at the beginning of the file\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "def validate_model(model, val_dataloader, device):\n",
        "    model.eval()  # Passer en mode évaluation\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Désactiver la rétropropagation pour la validation\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].long().to(device)\n",
        "\n",
        "            # Obtenir les prédictions du modèle\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Appliquer torch.argmax pour obtenir la classe prédite (classe 0 ou 1)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)  # dim=-1 car dernière dimension correspond aux classes\n",
        "\n",
        "            # Aplatir les prédictions et les labels pour calculer les métriques\n",
        "            all_preds.append(predictions.view(-1).cpu())  # Aplatir les prédictions\n",
        "            all_labels.append(labels.view(-1).cpu())  # Aplatir les labels\n",
        "\n",
        "    # Concaténer toutes les prédictions et les labels pour les métriques\n",
        "    all_preds = torch.cat(all_preds).cpu().numpy()  # Aplatir et convertir en numpy\n",
        "    all_labels = torch.cat(all_labels).cpu().numpy()  # Aplatir et convertir en numpy\n",
        "\n",
        "    # Calculer les métriques de validation (accuracy, recall, F1)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
        "\n",
        "    # Afficher les dimensions pour s'assurer qu'elles sont correctes\n",
        "    print(f\"Nombre de prédictions : {len(all_preds)}\")\n",
        "    print(f\"Nombre de labels : {len(all_labels)}\")\n",
        "\n",
        "    # Vérification des dimensions\n",
        "    if len(all_preds) != len(all_labels):\n",
        "        raise ValueError(f\"Inconsistent number of samples: {len(all_preds)} predictions vs {len(all_labels)} labels\")\n",
        "\n",
        "    # Retourner les métriques\n",
        "    return accuracy, recall, f1"
      ],
      "metadata": {
        "id": "fhLJqN2w_2AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Utilisation de l'optimiseur avec régularisation L2 (weight decay)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "# Entraînement et validation\n",
        "for epoch in range(10):  # Vous pouvez augmenter le nombre d'époques si nécessaire\n",
        "    print(f\"\\nÉpoque {epoch + 1}\\n{'=' * 20}\")\n",
        "\n",
        "    # Phase d'entraînement\n",
        "    avg_train_loss = train_model(model, train_dataloader, optimizer, device)\n",
        "\n",
        "    # Phase de validation\n",
        "    accuracy, recall, f1 = validate_model(model, val_dataloader, device)\n",
        "\n",
        "    print(f\"Résultats de l'époque {epoch + 1} :\")\n",
        "    print(f\"  Perte d'entraînement moyenne : {avg_train_loss}\")\n",
        "    print(f\"  Accuracy : {accuracy}, Recall : {recall}, F1-score : {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVVKEI1j_1-V",
        "outputId": "7b344d2f-0b6d-443b-974c-e31aa5158746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Époque 1\n",
            "====================\n",
            "Perte d'entraînement moyenne : 2.084542845686277\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 1 :\n",
            "  Perte d'entraînement moyenne : 2.084542845686277\n",
            "  Accuracy : 0.8681640625, Recall : 0.8681640625, F1-score : 0.8732369166470634\n",
            "\n",
            "Époque 2\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.2641853168606758\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 2 :\n",
            "  Perte d'entraînement moyenne : 0.2641853168606758\n",
            "  Accuracy : 0.8927556818181818, Recall : 0.8927556818181818, F1-score : 0.8960017023017987\n",
            "\n",
            "Époque 3\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.22762001554171243\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 3 :\n",
            "  Perte d'entraînement moyenne : 0.22762001554171243\n",
            "  Accuracy : 0.8937322443181818, Recall : 0.8937322443181818, F1-score : 0.8979825127618217\n",
            "\n",
            "Époque 4\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.17383977274099985\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 4 :\n",
            "  Perte d'entraînement moyenne : 0.17383977274099985\n",
            "  Accuracy : 0.8798828125, Recall : 0.8798828125, F1-score : 0.8855511210765911\n",
            "\n",
            "Époque 5\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.19297443081935248\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 5 :\n",
            "  Perte d'entraînement moyenne : 0.19297443081935248\n",
            "  Accuracy : 0.8831676136363636, Recall : 0.8831676136363636, F1-score : 0.888493804735311\n",
            "\n",
            "Époque 6\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.19884557028611502\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 6 :\n",
            "  Perte d'entraînement moyenne : 0.19884557028611502\n",
            "  Accuracy : 0.8789950284090909, Recall : 0.8789950284090909, F1-score : 0.8847674213839927\n",
            "\n",
            "Époque 7\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.18762685606877008\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 7 :\n",
            "  Perte d'entraînement moyenne : 0.18762685606877008\n",
            "  Accuracy : 0.8833451704545454, Recall : 0.8833451704545454, F1-score : 0.8887957440070557\n",
            "\n",
            "Époque 8\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.14549491430322328\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 8 :\n",
            "  Perte d'entraînement moyenne : 0.14549491430322328\n",
            "  Accuracy : 0.9452237215909091, Recall : 0.9452237215909091, F1-score : 0.9464297120265048\n",
            "\n",
            "Époque 9\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.1069221297899882\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 9 :\n",
            "  Perte d'entraînement moyenne : 0.1069221297899882\n",
            "  Accuracy : 0.9469992897727273, Recall : 0.9469992897727273, F1-score : 0.9482325299079982\n",
            "\n",
            "Époque 10\n",
            "====================\n",
            "Perte d'entraînement moyenne : 0.07752004141608874\n",
            "Nombre de prédictions : 11264\n",
            "Nombre de labels : 11264\n",
            "Résultats de l'époque 10 :\n",
            "  Perte d'entraînement moyenne : 0.07752004141608874\n",
            "  Accuracy : 0.9585404829545454, Recall : 0.9585404829545454, F1-score : 0.959077064930571\n"
          ]
        }
      ]
    }
  ]
}