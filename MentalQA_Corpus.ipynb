{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('/content/MentalQA_500_data.csv', sep='\\t')\n",
        "\n",
        "# Concaténer toutes les questions et réponses en une seule chaîne de texte\n",
        "all_text = ' '.join(data['question'].astype(str) + ' ' + data['answer'].astype(str))\n",
        "\n",
        "# Diviser le texte en mots et utiliser un ensemble pour les mots uniques\n",
        "vocabulary = set(all_text.split())  # Ensemble des mots uniques\n",
        "\n",
        "# Taille du vocabulaire\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Affichage du résultat\n",
        "print(f\"Taille du vocabulaire unique : {vocab_size}\")\n"
      ],
      "metadata": {
        "id": "o9nn_7dYg7x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKYSW_TSPmVw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('/content/MentalQA_500_data.csv', sep='\\t')  # Assure-toi que le chemin et le séparateur sont corrects\n",
        "\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire les questions et réponses de ton DataFrame\n",
        "questions = data['question'].tolist()\n",
        "answers = data['answer'].tolist()\n",
        "labels = data['final_AS'].apply(lambda x: int(x[2]) - 1).tolist()  # Assure-toi que les labels sont bien extraits"
      ],
      "metadata": {
        "id": "jmcDCanoPxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "pk5DPYLDQHQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "# Charger le tokenizer et modèle XLM-Roberta\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=3)"
      ],
      "metadata": {
        "id": "R9cMtOpHP4Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle Sentence-BERT pour l'analyse sémantique\n",
        "semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Classe pour préparer les données avec analyse sémantique\n",
        "class SegmentedDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.segments = []\n",
        "        self.segment_labels = []\n",
        "        self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        for text, label in zip(self.texts, self.labels):\n",
        "            question, answer = text, text\n",
        "            combined_text = self.analyse_semantique(question, answer)\n",
        "\n",
        "            tokens = self.tokenizer.encode(combined_text, truncation=False)\n",
        "            for i in range(0, len(tokens), self.max_length):\n",
        "                segment = tokens[i:i+self.max_length]\n",
        "                self.segments.append(segment)\n",
        "                self.segment_labels.append(label)\n",
        "\n",
        "    def analyse_semantique(self, question, answer):\n",
        "        # Diviser la réponse en segments (par exemple, par phrases)\n",
        "        segments = answer.split('.')\n",
        "        segments = [segment.strip() for segment in segments if segment]\n",
        "\n",
        "        # Encoder la question et les segments avec Sentence-BERT\n",
        "        question_embedding = semantic_model.encode(question, convert_to_tensor=True)\n",
        "        segment_embeddings = semantic_model.encode(segments, convert_to_tensor=True)\n",
        "\n",
        "        # Calculer la similarité cosinus\n",
        "        similarities = util.pytorch_cos_sim(question_embedding, segment_embeddings)\n",
        "\n",
        "        # Trier les segments par pertinence\n",
        "        segments_tries = [x for _, x in sorted(zip(similarities.tolist()[0], segments), reverse=True)]\n",
        "\n",
        "        # Combiner les segments triés pour former une réponse plus pertinente\n",
        "        combined_text = ' '.join(segments_tries)\n",
        "\n",
        "        return combined_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = torch.tensor(self.segments[idx])\n",
        "        label = torch.tensor(self.segment_labels[idx])\n",
        "\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        padding_length = self.max_length - input_ids.size(0)\n",
        "        if padding_length > 0:\n",
        "            input_ids = torch.cat([input_ids, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
        "            attention_mask = torch.cat([attention_mask, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': label\n",
        "        }"
      ],
      "metadata": {
        "id": "R_2P3xUkP7M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer le dataset et le dataloader avec tes données\n",
        "dataset = SegmentedDataset(questions, labels, tokenizer, max_length=512)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "hw9ZS3kYQJNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(dataloader) * 3\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UlRf_CXzQJG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, recall_score, accuracy_score"
      ],
      "metadata": {
        "id": "beJeryX6QWia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boucle d'entraînement\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    # Calcul des métriques à la fin de l'époque\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Époque {epoch+1}/3 - Perte Moyenne: {avg_loss:.4f} - Précision: {accuracy:.4f} - Rappel: {recall:.4f} - F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "zjFLd8FQQOii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIiz2IcYQOgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}